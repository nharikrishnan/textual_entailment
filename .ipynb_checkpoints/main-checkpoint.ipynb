{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing required functions from keras\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "\n",
    "#importing libraries required for preprocessing\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# importing the required util function\n",
    "import functions.utils\n",
    "from functions.utils import *\n",
    "from functions.utils import clean_text\n",
    "from functions.utils import load_embedding, create_embedding_weights, max_seq_len\n",
    "from keras.utils import to_categorical, plot_model\n",
    "\n",
    "import functions.model_evaluation\n",
    "from functions.model_evaluation import *\n",
    "\n",
    "import functions.data_manipulation\n",
    "from functions.data_manipulation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading datasets\n",
    "train = pd.read_csv('data/train.txt', delimiter = '\\t')\n",
    "test = pd.read_csv('data/test.txt', delimiter = '\\t')\n",
    "trial = pd.read_csv('data/trial.txt', delimiter = '\\t')\n",
    "test_ann = pd.read_csv('data/test_anno.txt', delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 32, 300)      692100      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300)          721200      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "L1_distance (Lambda)            (None, 300)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "entailment_task (Dense)         (None, 3)            903         L1_distance[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,414,203\n",
      "Trainable params: 722,103\n",
      "Non-trainable params: 692,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "4500/4500 [==============================] - 78s 17ms/step - loss: 0.7703 - accuracy: 0.6102 - val_loss: 0.6592 - val_accuracy: 0.6380\n",
      "Epoch 2/10\n",
      "4500/4500 [==============================] - 77s 17ms/step - loss: 0.5735 - accuracy: 0.7389 - val_loss: 0.5442 - val_accuracy: 0.7840s: 0.5739 - accuracy: 0.73\n",
      "Epoch 3/10\n",
      "4500/4500 [==============================] - 78s 17ms/step - loss: 0.4338 - accuracy: 0.8300 - val_loss: 0.5089 - val_accuracy: 0.8060\n",
      "Epoch 4/10\n",
      "4500/4500 [==============================] - 78s 17ms/step - loss: 0.3345 - accuracy: 0.8769 - val_loss: 0.5119 - val_accuracy: 0.8060\n",
      "Epoch 5/10\n",
      "4500/4500 [==============================] - 77s 17ms/step - loss: 0.2629 - accuracy: 0.9062 - val_loss: 0.4865 - val_accuracy: 0.7900\n",
      "Epoch 6/10\n",
      "4500/4500 [==============================] - 77s 17ms/step - loss: 0.1994 - accuracy: 0.9320 - val_loss: 0.4861 - val_accuracy: 0.8220\n",
      "Epoch 7/10\n",
      "4500/4500 [==============================] - 77s 17ms/step - loss: 0.1482 - accuracy: 0.9538 - val_loss: 0.5569 - val_accuracy: 0.7920\n",
      "Epoch 8/10\n",
      "4500/4500 [==============================] - 64s 14ms/step - loss: 0.1094 - accuracy: 0.9687 - val_loss: 0.5244 - val_accuracy: 0.8240\n",
      "Epoch 9/10\n",
      "4500/4500 [==============================] - 60s 13ms/step - loss: 0.0792 - accuracy: 0.9820 - val_loss: 0.5860 - val_accuracy: 0.8100\n",
      "Epoch 10/10\n",
      "4500/4500 [==============================] - 60s 13ms/step - loss: 0.0643 - accuracy: 0.9833 - val_loss: 0.5689 - val_accuracy: 0.8220\n"
     ]
    }
   ],
   "source": [
    "#Cleaning the data\n",
    "train['sentence_A'] = train['sentence_A'].apply(clean_text)\n",
    "train['sentence_B'] = train['sentence_B'].apply(clean_text)\n",
    "test['sentence_A'] = test['sentence_A'].apply(clean_text)\n",
    "test['sentence_B'] = test['sentence_B'].apply(clean_text)\n",
    "trial['sentence_A'] = trial['sentence_A'].apply(clean_text)\n",
    "trial['sentence_B'] = trial['sentence_B'].apply(clean_text)\n",
    "\n",
    "#encoding the target feature\n",
    "lbl_enc = LabelEncoder()\n",
    "train['entailment_encoded'] = lbl_enc.fit_transform(train['entailment_judgment'])\n",
    "trial['entailment_encoded'] = lbl_enc.fit_transform(trial['entailment_judgment'])\n",
    "############################For now############################################\n",
    "test_ann['entailment_encoded'] = lbl_enc.fit_transform(test_ann['entailment_judgment'])\n",
    "\n",
    "#loading the embedding\n",
    "file_name = 'word_embeddings/glove.6B.300d.txt'\n",
    "embeddings = load_embedding(file_name)\n",
    "\n",
    "\n",
    "#tokenizing \n",
    "NUM_WORDS = len(embeddings) #200000\n",
    "sentences = (list(train['sentence_A']) + list(train['sentence_B']) + \n",
    "                       list(test['sentence_A']) + list(test['sentence_B'])+ \n",
    "                           list(trial['sentence_A']) + list(trial['sentence_B']))\n",
    "tokenize = Tokenizer(num_words = NUM_WORDS)\n",
    "tokenize.fit_on_texts(sentences)\n",
    "sent1_word_seq = tokenize.texts_to_sequences(train['sentence_A'])\n",
    "sent2_word_seq = tokenize.texts_to_sequences(train['sentence_B'])\n",
    "sent1_word_seq_test = tokenize.texts_to_sequences(test['sentence_A'])\n",
    "sent2_word_seq_test = tokenize.texts_to_sequences(test['sentence_B'])\n",
    "sent1_word_seq_trial = tokenize.texts_to_sequences(trial['sentence_A'])\n",
    "sent2_word_seq_trial = tokenize.texts_to_sequences(trial['sentence_B'])\n",
    "word_index = tokenize.word_index\n",
    "\n",
    "#Matrix with the embedding weights\n",
    "embedding_dim = 300\n",
    "embedding_weights = create_embedding_weights(embeddings, embedding_dim, word_index, NUM_WORDS)\n",
    "\n",
    "# extracting the maximum sequence length\n",
    "max_seq_length = max_seq_len(sent1_word_seq)\n",
    "max_seq_length = max_seq_len(sent2_word_seq, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent1_word_seq_test, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent2_word_seq_test, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent1_word_seq_trial, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent2_word_seq_trial, max_seq_length)\n",
    "\n",
    "# padding the sequences\n",
    "sent1_data = pad_sequences(sent1_word_seq, maxlen = max_seq_length)\n",
    "sent2_data = pad_sequences(sent2_word_seq, maxlen = max_seq_length)\n",
    "\n",
    "sent1_data_trial = pad_sequences(sent1_word_seq_trial, maxlen = max_seq_length)\n",
    "sent2_data_trial = pad_sequences(sent2_word_seq_trial, maxlen = max_seq_length)\n",
    "\n",
    "sent1_data_test = pad_sequences(sent1_word_seq_test, maxlen = max_seq_length)\n",
    "sent2_data_test = pad_sequences(sent2_word_seq_test, maxlen = max_seq_length)\n",
    "NUM_WORDS = len(embedding_weights)\n",
    "\n",
    "labels = to_categorical(np.asarray(train['entailment_encoded']))\n",
    "labels_valid = to_categorical(np.asarray(trial['entailment_encoded']))\n",
    "\n",
    "\n",
    "### Running the model\n",
    "\n",
    "# number of iterations, batch_size and validation split for the model\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "#laoding the module from deep learning models\n",
    "import models.models as models\n",
    "\n",
    "m1 = models.models(embedding_dim,\n",
    "                   NUM_WORDS, \n",
    "                   embedding_weights,\n",
    "#                   dropout = .1,\n",
    "#                   l2_reg = .0001\n",
    "                   max_seq_length,\n",
    "                  )\n",
    "\n",
    "model = m1.siames()\n",
    "\n",
    "#training the model\n",
    "hist = model.fit([sent1_data, sent2_data], labels, batch_size = batch_size, \n",
    "                 epochs = epochs,\n",
    "                 validation_data= ([sent1_data_trial, sent2_data_trial],labels_valid ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = model.predict([sent1_data_test, sent2_data_test])\n",
    "result = convert_prob(k)\n",
    "prec, rec, f1_s, acc = evaluate_model(test_ann['entailment_encoded'], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8237316907582141\n",
      "Recall: 0.8234219606251268\n",
      "F1 Score: 0.823533660232343\n",
      "Accuracy: 0.8234219606251268\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: \"+ str(prec))\n",
    "print(\"Recall: \"+ str(rec))\n",
    "print(\"F1 Score: \"+ str(f1_s))\n",
    "print(\"Accuracy: \"+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hkpat\\Masters\\NLP\\Assignment2_final\\submission\\functions\\data_manipulation.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['entailment_judgment'] =  entailment_result\n"
     ]
    }
   ],
   "source": [
    "df = map_entailment(lbl_enc, result, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 32, 300)      692100      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 300)          721200      embedding_2[0][0]                \n",
      "                                                                 embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "L1_distance (Lambda)            (None, 300)          0           lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 300)          0           L1_distance[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "similarity_task (Dense)         (None, 1)            301         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,413,601\n",
      "Trainable params: 721,501\n",
      "Non-trainable params: 692,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# loading the model\n",
    "m1 = models.models(embedding_dim = embedding_dim,\n",
    "                   NUM_WORDS = NUM_WORDS,\n",
    "                   embedding_weights = embedding_weights,\n",
    "                   max_seq_length = max_seq_length,\n",
    "                   task = 'relatedness',\n",
    "                   dropout = .1,\n",
    "                   l2_reg=.0001\n",
    "                  )\n",
    "model = m1.siames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "4500/4500 [==============================] - 60s 13ms/step - loss: 0.9722 - pearson_correlation: 0.4094 - val_loss: 0.7397 - val_pearson_correlation: 0.6312\n",
      "Epoch 2/10\n",
      "4500/4500 [==============================] - 60s 13ms/step - loss: 0.5909 - pearson_correlation: 0.6545 - val_loss: 0.4766 - val_pearson_correlation: 0.6771\n",
      "Epoch 3/10\n",
      "4500/4500 [==============================] - 60s 13ms/step - loss: 0.4645 - pearson_correlation: 0.7400 - val_loss: 0.4331 - val_pearson_correlation: 0.7388\n",
      "Epoch 4/10\n",
      "4500/4500 [==============================] - 60s 13ms/step - loss: 0.3746 - pearson_correlation: 0.7934 - val_loss: 0.4035 - val_pearson_correlation: 0.7532\n",
      "Epoch 5/10\n",
      "4500/4500 [==============================] - 60s 13ms/step - loss: 0.3241 - pearson_correlation: 0.8246 - val_loss: 0.3502 - val_pearson_correlation: 0.7512\n",
      "Epoch 6/10\n",
      "4500/4500 [==============================] - 60s 13ms/step - loss: 0.2686 - pearson_correlation: 0.8629 - val_loss: 0.5729 - val_pearson_correlation: 0.7520\n",
      "Epoch 7/10\n",
      "4500/4500 [==============================] - 60s 13ms/step - loss: 0.2407 - pearson_correlation: 0.8716 - val_loss: 0.4183 - val_pearson_correlation: 0.7683\n",
      "Epoch 8/10\n",
      "4500/4500 [==============================] - 60s 13ms/step - loss: 0.2119 - pearson_correlation: 0.8952 - val_loss: 0.4649 - val_pearson_correlation: 0.7769\n",
      "Epoch 9/10\n",
      "4500/4500 [==============================] - 60s 13ms/step - loss: 0.1773 - pearson_correlation: 0.9037 - val_loss: 0.3654 - val_pearson_correlation: 0.7575\n",
      "Epoch 10/10\n",
      "4500/4500 [==============================] - 60s 13ms/step - loss: 0.1629 - pearson_correlation: 0.9177 - val_loss: 0.3409 - val_pearson_correlation: 0.7577\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([sent1_data, sent2_data], train['relatedness_score'], batch_size = 10, \n",
    "                 epochs = 10,\n",
    "                 validation_data = ([sent1_data_trial, sent2_data_trial], trial['relatedness_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = model.predict([sent1_data_test, sent2_data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [i[0] for i in k]\n",
    "y_true = test_ann['relatedness_score']\n",
    "pearson, spearman, mean_abs_deviation = evaluate_relatedness(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson corr: 0.8243550881303399\n",
      "spearman corr: 0.7724235457409069\n",
      "mean_absolute_deviation: 15.748742275484046\n"
     ]
    }
   ],
   "source": [
    "print(\"pearson corr: \"+str(pearson[0]))\n",
    "print(\"spearman corr: \"+str(spearman[0]))\n",
    "print(\"mean_absolute_deviation: \" +str(mean_abs_deviation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'functions.data_manipulation' from 'C:\\\\Users\\\\hkpat\\\\Masters\\\\NLP\\\\Assignment2_final\\\\submission\\\\functions\\\\data_manipulation.py'>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(functions.data_manipulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.data_manipulation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = map_relatedness(df, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>entailment_judgment</th>\n",
       "      <th>relatedness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2.684834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>3.765484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>3.531833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>4.649788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>4.382194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_ID entailment_judgment  relatedness_score\n",
       "0  6        NEUTRAL             2.684834         \n",
       "1  7        NEUTRAL             3.765484         \n",
       "2  8        NEUTRAL             3.531833         \n",
       "3  10       ENTAILMENT          4.649788         \n",
       "4  11       ENTAILMENT          4.382194         "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv(df_output, \"output/result.txt\", index = False)\n",
    "# to write it to the folder from which the code is being executed\n",
    "#output_csv(df_output, \"result.txt\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
