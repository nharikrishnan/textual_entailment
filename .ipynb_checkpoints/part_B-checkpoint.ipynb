{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import functions.utils\n",
    "from functions.utils import *\n",
    "from functions.utils import clean_text\n",
    "from functions.utils import load_embedding, create_embedding_weights, max_seq_len\n",
    "from keras.utils import to_categorical, plot_model\n",
    "import keras.backend as K\n",
    "import matplotlib as plt\n",
    "import models.models\n",
    "\n",
    "import functions.model_evaluation\n",
    "from functions.model_evaluation import *\n",
    "\n",
    "import functions.data_manipulation\n",
    "from functions.data_manipulation import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading datasets\n",
    "#df = pd.read_csv('../sick.csv')\n",
    "train = pd.read_csv('data/train.txt', delimiter = '\\t')\n",
    "test = pd.read_csv('data/test_anno.txt', delimiter = '\\t')\n",
    "trial = pd.read_csv('data/trial.txt', delimiter = '\\t')\n",
    "#train, test = train_test_split(df, test_size=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data\n",
    "train['sentence_A'] = train['sentence_A'].apply(clean_text)\n",
    "train['sentence_B'] = train['sentence_B'].apply(clean_text)\n",
    "test['sentence_A'] = test['sentence_A'].apply(clean_text)\n",
    "test['sentence_B'] = test['sentence_B'].apply(clean_text)\n",
    "trial['sentence_A'] = trial['sentence_A'].apply(clean_text)\n",
    "trial['sentence_B'] = trial['sentence_B'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding the target feature\n",
    "lbl_enc = LabelEncoder()\n",
    "train['entailment_encoded'] = lbl_enc.fit_transform(train['entailment_judgment'])\n",
    "trial['entailment_encoded'] = lbl_enc.fit_transform(trial['entailment_judgment'])\n",
    "############################For now############################################\n",
    "test['entailment_encoded'] = lbl_enc.fit_transform(test['entailment_judgment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'word_embeddings/glove.6B.300d.txt'\n",
    "embeddings = load_embedding(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = len(embeddings) #200000\n",
    "sentences = (list(train['sentence_A']) + list(train['sentence_B']) + \n",
    "                       list(test['sentence_A']) + list(test['sentence_B'])+ \n",
    "                           list(trial['sentence_A']) + list(trial['sentence_B']))\n",
    "tokenize = Tokenizer(num_words = NUM_WORDS)\n",
    "tokenize.fit_on_texts(sentences)\n",
    "sent1_word_seq = tokenize.texts_to_sequences(train['sentence_A'])\n",
    "sent2_word_seq = tokenize.texts_to_sequences(train['sentence_B'])\n",
    "sent1_word_seq_test = tokenize.texts_to_sequences(test['sentence_A'])\n",
    "sent2_word_seq_test = tokenize.texts_to_sequences(test['sentence_B'])\n",
    "sent1_word_seq_trial = tokenize.texts_to_sequences(trial['sentence_A'])\n",
    "sent2_word_seq_trial = tokenize.texts_to_sequences(trial['sentence_B'])\n",
    "word_index = tokenize.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrix with the embedding weights\n",
    "embedding_dim = 300\n",
    "embedding_weights = create_embedding_weights(embeddings, embedding_dim, word_index, NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the maximum sequence length\n",
    "max_seq_length = max_seq_len(sent1_word_seq)\n",
    "max_seq_length = max_seq_len(sent2_word_seq, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent1_word_seq_test, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent2_word_seq_test, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent1_word_seq_trial, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent2_word_seq_trial, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the sequences\n",
    "sent1_data = pad_sequences(sent1_word_seq, maxlen = max_seq_length)\n",
    "sent2_data = pad_sequences(sent2_word_seq, maxlen = max_seq_length)\n",
    "\n",
    "sent1_data_trial = pad_sequences(sent1_word_seq_trial, maxlen = max_seq_length)\n",
    "sent2_data_trial = pad_sequences(sent2_word_seq_trial, maxlen = max_seq_length)\n",
    "\n",
    "sent1_data_test = pad_sequences(sent1_word_seq_test, maxlen = max_seq_length)\n",
    "sent2_data_test = pad_sequences(sent2_word_seq_test, maxlen = max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2307"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_WORDS = len(embedding_weights)\n",
    "NUM_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Lambda, Concatenate, Flatten\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.optimizers import Adam, Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_train = np.concatenate((sent1_data, sent2_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 64, 300)      692100      main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64, 300)      0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_fwd (LSTM)                 (None, 64, 125)      213000      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_bwd (LSTM)                 (None, 64, 125)      213000      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 250)      0           lstm_fwd[0][0]                   \n",
      "                                                                 lstm_bwd[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "d_bilstm (Dropout)              (None, 64, 250)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flat_h_star (Flatten)           (None, 16000)        0           d_bilstm[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            16001       flat_h_star[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,134,101\n",
      "Trainable params: 442,001\n",
      "Non-trainable params: 692,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hkpat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "# k = 2 * opts['lstm_units']  # 200\n",
    "# L = opts['xmaxlen']  # 35\n",
    "# N = opts['xmaxlen'] + opts['ymaxlen']\n",
    "\n",
    "main_input = Input(shape=(max_seq_length,), dtype='int32', name='main_input') #(N,70)\n",
    "#x = Embedding(output_dim=opts['emb'], input_dim=len(VOCABULARY.keys())+1, input_length=N, name='x')(main_input)\n",
    "\n",
    "x = Embedding(\n",
    "                NUM_WORDS,\n",
    "                embedding_dim,\n",
    "                weights = [embedding_weights], \n",
    "                input_length = max_seq_length,\n",
    "                trainable = False)(main_input)\n",
    "\n",
    "drop_out = Dropout(0.3, name='dropout')(x) # 70,50\n",
    "lstm_fwd = LSTM(125, return_sequences=True, name='lstm_fwd')(drop_out)\n",
    "lstm_bwd = LSTM(125, return_sequences=True, go_backwards=True, name='lstm_bwd')(drop_out)\n",
    "#70,100\n",
    "bilstm = Concatenate()([lstm_fwd,lstm_bwd])\n",
    "#70,200\n",
    "drop_out = Dropout(0.1, name=\"d_bilstm\")(bilstm)\n",
    "flat_h_star = Flatten(name=\"flat_h_star\")(drop_out)\n",
    "out = Dense(1, activation='selu')(flat_h_star)\n",
    "\n",
    "model = Model(input=[main_input], output=out)\n",
    "model.summary()\n",
    "    # plot(model, 'model.png')\n",
    "    # # model.compile(loss={'output':'binary_crossentropy'}, optimizer=Adam())\n",
    "    # model.compile(loss={'output':'categorical_crossentropy'}, optimizer=Adam(options.lr))\n",
    "    \n",
    "model.compile(loss='MSE',optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4050 samples, validate on 450 samples\n",
      "Epoch 1/10\n",
      "4050/4050 [==============================] - 17s 4ms/step - loss: 1.2555 - val_loss: 1.9454\n",
      "Epoch 2/10\n",
      "4050/4050 [==============================] - 16s 4ms/step - loss: 0.9873 - val_loss: 1.9853\n",
      "Epoch 3/10\n",
      "4050/4050 [==============================] - 16s 4ms/step - loss: 0.9262 - val_loss: 1.8631\n",
      "Epoch 4/10\n",
      "4050/4050 [==============================] - 15s 4ms/step - loss: 0.8787 - val_loss: 1.9876\n",
      "Epoch 5/10\n",
      "4050/4050 [==============================] - 12s 3ms/step - loss: 0.8678 - val_loss: 1.6753\n",
      "Epoch 6/10\n",
      "4050/4050 [==============================] - 15s 4ms/step - loss: 0.8247 - val_loss: 2.1948\n",
      "Epoch 7/10\n",
      "4050/4050 [==============================] - 16s 4ms/step - loss: 0.7954 - val_loss: 1.7964\n",
      "Epoch 8/10\n",
      "4050/4050 [==============================] - 16s 4ms/step - loss: 0.7159 - val_loss: 1.8933\n",
      "Epoch 9/10\n",
      "4050/4050 [==============================] - 15s 4ms/step - loss: 0.6946 - val_loss: 1.9010\n",
      "Epoch 10/10\n",
      "4050/4050 [==============================] - 14s 3ms/step - loss: 0.6682 - val_loss: 2.4417\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(xy_train, train['relatedness_score'], batch_size = 32, \n",
    "                 epochs = 10,\n",
    "                 validation_split = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_train2 = np.concatenate((sent1_data_test, sent2_data_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = model.predict(xy_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [i[0] for i in k]\n",
    "y_true = test['relatedness_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson, spearman, mean_abs_deviation = evaluate_relatedness(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson: 0.1999905092359735\n",
      "spearman: 0.1821493033889926\n",
      "mean_abs_deviation: 36.19199897591299\n"
     ]
    }
   ],
   "source": [
    "print(\"pearson: \"+str(pearson[0]))\n",
    "print(\"spearman: \"+str(spearman[0]))\n",
    "print(\"mean_abs_deviation: \"+str(mean_abs_deviation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.678286661869105"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(test['relatedness_score'], test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(left,right):\n",
    "    left = K.l2_normalize(left, axis=-1)\n",
    "    right = K.l2_normalize(right, axis=-1)\n",
    "    return -K.mean(left * right, axis=-1, keepdims=True)\n",
    "\n",
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "def pearson_correlation(y_true, y_pred):\n",
    "    # Pearson's correlation coefficient = covariance(X, Y) / (stdv(X) * stdv(Y))\n",
    "    fs_pred = y_pred - K.mean(y_pred)\n",
    "    fs_true = y_true - K.mean(y_true)\n",
    "    covariance = K.mean(fs_true * fs_pred)\n",
    "    \n",
    "    stdv_true = K.std(y_true)\n",
    "    stdv_pred = K.std(y_pred)\n",
    "    \n",
    "    return covariance / (stdv_true * stdv_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "    def __init__(self, NUM_WORDS, embedding_dim, weights, max_seq_length):\n",
    "        self.NUM_WORDS = NUM_WORDS\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.weights = weights\n",
    "        self.max_seq_lenght = max_seq_lenght\n",
    "    def siames_lstm():\n",
    "        #LSTM layer\n",
    "        lstm = layers.LSTM(embedding_dim)\n",
    "        \n",
    "        # Embedding layer with the embedding weights: shared weights\n",
    "        embedding_layer = Embedding(\n",
    "                self.NUM_WORDS,\n",
    "                self.embedding_dim,\n",
    "                weights = [self.embedding_weights], \n",
    "                input_length = self.max_seq_length,\n",
    "                trainable = False)\n",
    "        \n",
    "        # Creating inputs\n",
    "        # Left input\n",
    "        left_input = Input(shape=(self.max_seq_length,), name='input_1')\n",
    "        left_output = embedding_layer(left_input)\n",
    "        left_output = layers.Dropout(.1)(left_output)\n",
    "        left_output = lstm(left_output)\n",
    "\n",
    "        # Right input\n",
    "        right_input = Input(shape=(self.max_seq_length,), name='input_2')\n",
    "        right_output = embedding_layer(right_input)\n",
    "        right_output = layers.Dropout(.1)(right_output)\n",
    "        right_output = lstm(right_output) \n",
    "        \n",
    "        # temporary\n",
    "        \n",
    "        l1_norm = lambda x: 1 - K.abs(x[0] - x[1])\n",
    "\n",
    "        # merging both the input using any distance calculation\n",
    "        merged = layers.Lambda(function=l1_norm, output_shape=lambda x: x[0], \n",
    "                                   name='L1_distance')([left_output, right_output])\n",
    "\n",
    "        # output layer\n",
    "        predictions = layers.Dense(1, activation='selu', name='Similarity_layer')(merged)\n",
    "\n",
    "        # Creating the model\n",
    "        model = Model([left_input, right_input], predictions)\n",
    "        \n",
    "        #compiling the model\n",
    "        model.compile(loss = 'mse', optimizer = Adam())\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from 'C:\\\\Users\\\\hkpat\\\\Masters\\\\NLP\\\\Assignment2_final\\\\models.py'>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = models.models(embedding_dim = embedding_dim,\n",
    "                   NUM_WORDS = NUM_WORDS,\n",
    "                   embedding_weights = embedding_weights,\n",
    "                   max_seq_length = max_seq_length,\n",
    "                   task = 'relatedness',\n",
    "                   dropout = .1\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_51 (Embedding)        (None, 32, 300)      692100      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                  (None, 300)          721200      embedding_51[0][0]               \n",
      "                                                                 embedding_51[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "L1_distance (Lambda)            (None, 300)          0           lstm_23[0][0]                    \n",
      "                                                                 lstm_23[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 300)          0           L1_distance[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "similarity_task (Dense)         (None, 1)            301         dropout_32[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,413,601\n",
      "Trainable params: 721,501\n",
      "Non-trainable params: 692,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = m1.siames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/13\n",
      "4500/4500 [==============================] - 74s 16ms/step - loss: 1.0962 - val_loss: 0.8022\n",
      "Epoch 2/13\n",
      "4500/4500 [==============================] - 72s 16ms/step - loss: 0.6631 - val_loss: 0.6342\n",
      "Epoch 3/13\n",
      "4500/4500 [==============================] - 72s 16ms/step - loss: 0.5041 - val_loss: 0.4402\n",
      "Epoch 4/13\n",
      "4500/4500 [==============================] - 1253s 278ms/step - loss: 0.4095 - val_loss: 0.3930\n",
      "Epoch 5/13\n",
      "4500/4500 [==============================] - 67s 15ms/step - loss: 0.3513 - val_loss: 0.3682\n",
      "Epoch 6/13\n",
      "4500/4500 [==============================] - 68s 15ms/step - loss: 0.2992 - val_loss: 0.4110\n",
      "Epoch 7/13\n",
      "4500/4500 [==============================] - 65s 14ms/step - loss: 0.2432 - val_loss: 0.3638\n",
      "Epoch 8/13\n",
      "4500/4500 [==============================] - 64s 14ms/step - loss: 0.2274 - val_loss: 0.3448\n",
      "Epoch 9/13\n",
      "4500/4500 [==============================] - 65s 14ms/step - loss: 0.2002 - val_loss: 0.3136\n",
      "Epoch 10/13\n",
      "4500/4500 [==============================] - 74s 16ms/step - loss: 0.1753 - val_loss: 0.3085\n",
      "Epoch 11/13\n",
      "4500/4500 [==============================] - 75s 17ms/step - loss: 0.1660 - val_loss: 0.4386\n",
      "Epoch 12/13\n",
      "4500/4500 [==============================] - 72s 16ms/step - loss: 0.1462 - val_loss: 0.3168\n",
      "Epoch 13/13\n",
      "4500/4500 [==============================] - 70s 16ms/step - loss: 0.1450 - val_loss: 0.5507\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([sent1_data, sent2_data], train['relatedness_score'], batch_size = 10, \n",
    "                 epochs = 13,\n",
    "                 validation_data = ([sent1_data_trial, sent2_data_trial], trial['relatedness_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = model.predict([sent1_data_test, sent2_data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for j in range(len(k)):\n",
    "    t.append(abs(k[j] - test['relatedness_score'][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = []\n",
    "for i in k.tolist():\n",
    "    test1.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8216860438392607, 0.0)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "stats.pearsonr(test1, np.array(test['relatedness_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.116068723958925"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(test1, test['relatedness_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer\n",
    "lstm = layers.LSTM(embedding_dim)\n",
    "# Embedding layer with the embedding weights\n",
    "embedding_layer = Embedding(\n",
    "        NUM_WORDS,\n",
    "        embedding_dim,\n",
    "        weights = [embedding_weights], \n",
    "        input_length = max_seq_length,\n",
    "        trainable = False)\n",
    "# Creating inputs\n",
    "\n",
    "# Left input\n",
    "left_input = Input(shape=(max_seq_length,), name='input_1')\n",
    "left_output = embedding_layer(left_input)\n",
    "left_output = layers.Dropout(.1)(left_output)\n",
    "left_output = lstm(left_output)\n",
    "\n",
    "# Right input\n",
    "right_input = Input(shape=(max_seq_length,), name='input_2')\n",
    "right_output = embedding_layer(right_input)\n",
    "right_output = layers.Dropout(.1)(right_output)\n",
    "right_output = lstm(right_output) \n",
    "\n",
    "# temporary\n",
    "l1_norm = lambda x: 1 - K.abs(x[0] - x[1])\n",
    "\n",
    "# merging both the input using any distance calculation\n",
    "merged = layers.Lambda(function=l1_norm, output_shape=lambda x: x[0], \n",
    "                               name='L1_distance')([left_output, right_output])\n",
    "\n",
    "# output layer\n",
    "predictions = layers.Dense(1, activation='selu', name='Similarity_layer')(merged)\n",
    "\n",
    "# Creating the model\n",
    "model = Model([left_input, right_input], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mse', optimizer = Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "4500/4500 [==============================] - 41s 9ms/step - loss: 0.9322 - val_loss: 0.6205\n",
      "Epoch 2/10\n",
      "4500/4500 [==============================] - 45s 10ms/step - loss: 0.4930 - val_loss: 0.5582\n",
      "Epoch 3/10\n",
      "4500/4500 [==============================] - 45s 10ms/step - loss: 0.3755 - val_loss: 0.4411\n",
      "Epoch 4/10\n",
      "4500/4500 [==============================] - 44s 10ms/step - loss: 0.2987 - val_loss: 0.3736\n",
      "Epoch 5/10\n",
      "4500/4500 [==============================] - 45s 10ms/step - loss: 0.2455 - val_loss: 0.3567\n",
      "Epoch 6/10\n",
      "4500/4500 [==============================] - 46s 10ms/step - loss: 0.2100 - val_loss: 0.3812\n",
      "Epoch 7/10\n",
      "4500/4500 [==============================] - 49s 11ms/step - loss: 0.1858 - val_loss: 0.3379\n",
      "Epoch 8/10\n",
      "4500/4500 [==============================] - 49s 11ms/step - loss: 0.1594 - val_loss: 0.3236\n",
      "Epoch 9/10\n",
      "4500/4500 [==============================] - 49s 11ms/step - loss: 0.1469 - val_loss: 0.3579\n",
      "Epoch 10/10\n",
      "4500/4500 [==============================] - 54s 12ms/step - loss: 0.1257 - val_loss: 0.3432\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([sent1_data, sent2_data], train['relatedness_score'], batch_size = 32, \n",
    "                 epochs = 10,\n",
    "                 validation_data = ([sent1_data_trial, sent2_data_trial], trial['relatedness_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = model.predict([sent1_data_test, sent2_data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for j in range(len(k)):\n",
    "    t.append(abs(k[j] - test['relatedness_score'][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.915497e-05"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.927041], dtype=float32)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       3.300\n",
       "1       3.700\n",
       "2       3.000\n",
       "3       4.900\n",
       "4       3.665\n",
       "        ...  \n",
       "4922    2.100\n",
       "4923    1.000\n",
       "4924    1.000\n",
       "4925    1.000\n",
       "4926    1.000\n",
       "Name: relatedness_score, Length: 4927, dtype: float64"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['relatedness_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if (TRUE %in% is.na(score$relatedness_score)){\n",
    "# \tprint(\"No data for the relatedness task: evaluation on entailment only \")\n",
    "# \t}else if (is.numeric(score$relatedness_score)==FALSE){\n",
    "# \tprint(\"ERROR: wrong format for relatedness scores\")\n",
    "# \t}else{\n",
    "# \tpearson <- cor(score$relatedness_score, gold$relatedness_score)\n",
    "# \tprint(paste(\"Relatedness: Pearson correlation \", pearson, sep=\"\"))\n",
    "# \tspearman <- cor(score$relatedness_score, gold$relatedness_score, method = \"spearman\")\n",
    "# \tprint(paste(\"Relatedness: Spearman correlation \", spearman, sep=\"\"))\n",
    "# \tMSE <- sum((score$relatedness_score - gold$relatedness_score)^2) / length(score$relatedness_score)\n",
    "# \tprint(paste(\"Relatedness: MSE \", MSE, sep=\"\"))\n",
    "# \t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = []\n",
    "for i in k.tolist():\n",
    "    test1.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.1787211894989014,\n",
       " 3.7040488719940186,\n",
       " 3.3612756729125977,\n",
       " 4.927041053771973,\n",
       " 4.539432525634766,\n",
       " 3.135829210281372,\n",
       " 3.7842445373535156,\n",
       " 3.3350541591644287,\n",
       " 3.4776501655578613,\n",
       " 4.855803489685059,\n",
       " 3.6978399753570557,\n",
       " 4.577384948730469,\n",
       " 4.842310905456543,\n",
       " 4.1223320960998535,\n",
       " 3.138801097869873,\n",
       " 3.5831353664398193,\n",
       " 3.9096391201019287,\n",
       " 4.527817726135254,\n",
       " 4.407266139984131,\n",
       " 4.091448783874512,\n",
       " 3.2629551887512207,\n",
       " 4.096042633056641,\n",
       " 3.0766685009002686,\n",
       " 4.071567535400391,\n",
       " 4.317993640899658,\n",
       " 4.756462574005127,\n",
       " 3.9820613861083984,\n",
       " 3.64382004737854,\n",
       " 3.8939762115478516,\n",
       " 4.064579963684082,\n",
       " 4.919336795806885,\n",
       " 3.952439785003662,\n",
       " 3.807729959487915,\n",
       " 3.8449606895446777,\n",
       " 3.667332649230957,\n",
       " 4.088071346282959,\n",
       " 3.390010118484497,\n",
       " 4.351051330566406,\n",
       " 3.678745746612549,\n",
       " 4.158207893371582,\n",
       " 4.0402727127075195,\n",
       " 3.585137128829956,\n",
       " 3.6356067657470703,\n",
       " 3.827357053756714,\n",
       " 4.9052324295043945,\n",
       " 4.368733882904053,\n",
       " 4.300821781158447,\n",
       " 2.492065906524658,\n",
       " 2.3041934967041016,\n",
       " 4.4356536865234375,\n",
       " 1.2002159357070923,\n",
       " 1.5167803764343262,\n",
       " 0.5969312787055969,\n",
       " 4.538374423980713,\n",
       " 3.865495443344116,\n",
       " 3.4802350997924805,\n",
       " 3.636293411254883,\n",
       " 3.6628658771514893,\n",
       " 3.93156099319458,\n",
       " 3.9889914989471436,\n",
       " 4.779531478881836,\n",
       " 4.637235164642334,\n",
       " 3.395179033279419,\n",
       " 3.221268653869629,\n",
       " 4.606736183166504,\n",
       " 4.645201206207275,\n",
       " 2.861907958984375,\n",
       " 3.189964532852173,\n",
       " 2.93864369392395,\n",
       " 3.2259013652801514,\n",
       " 3.727489948272705,\n",
       " 3.4181902408599854,\n",
       " 4.049057483673096,\n",
       " 3.4361183643341064,\n",
       " 4.215031147003174,\n",
       " 3.9741997718811035,\n",
       " 3.4505221843719482,\n",
       " 2.722360372543335,\n",
       " 3.1514902114868164,\n",
       " 4.2882890701293945,\n",
       " 4.530777454376221,\n",
       " 3.2435760498046875,\n",
       " 4.407391548156738,\n",
       " 3.3084309101104736,\n",
       " 4.242650508880615,\n",
       " 3.217843770980835,\n",
       " 4.758817195892334,\n",
       " 4.923457622528076,\n",
       " 4.811369895935059,\n",
       " 4.0945611000061035,\n",
       " 3.763470411300659,\n",
       " 3.7056870460510254,\n",
       " 4.2697954177856445,\n",
       " 3.4239933490753174,\n",
       " 4.836164474487305,\n",
       " 4.673058986663818,\n",
       " 3.9114346504211426,\n",
       " 3.9340243339538574,\n",
       " 4.0077385902404785,\n",
       " 3.7499566078186035,\n",
       " 4.383214473724365,\n",
       " 3.483215570449829,\n",
       " 4.630513668060303,\n",
       " 4.711801052093506,\n",
       " 4.378641605377197,\n",
       " 3.167987585067749,\n",
       " 3.4068024158477783,\n",
       " 3.4410951137542725,\n",
       " 4.0957818031311035,\n",
       " 4.223878860473633,\n",
       " 3.4241297245025635,\n",
       " 3.7083585262298584,\n",
       " 3.6319501399993896,\n",
       " 3.732760429382324,\n",
       " 3.6289374828338623,\n",
       " 3.7933731079101562,\n",
       " 3.8294382095336914,\n",
       " 3.563772439956665,\n",
       " 3.9719948768615723,\n",
       " 4.200907230377197,\n",
       " 3.452627420425415,\n",
       " 3.897364377975464,\n",
       " 3.1588046550750732,\n",
       " 2.6670005321502686,\n",
       " 4.415744304656982,\n",
       " 4.763416767120361,\n",
       " 3.917423963546753,\n",
       " 4.643499851226807,\n",
       " 3.70182204246521,\n",
       " 4.673616409301758,\n",
       " 2.576333522796631,\n",
       " 2.47914457321167,\n",
       " 4.650443077087402,\n",
       " 4.220881938934326,\n",
       " 3.6946678161621094,\n",
       " 4.746056079864502,\n",
       " 3.568283796310425,\n",
       " 3.2940030097961426,\n",
       " 3.814718008041382,\n",
       " 3.822857141494751,\n",
       " 3.7346622943878174,\n",
       " 4.523568153381348,\n",
       " 2.925715208053589,\n",
       " 4.237274169921875,\n",
       " 3.974846363067627,\n",
       " 4.046380996704102,\n",
       " 3.9778010845184326,\n",
       " 3.470388650894165,\n",
       " 4.024472713470459,\n",
       " 4.467862606048584,\n",
       " 4.562102317810059,\n",
       " 4.4872236251831055,\n",
       " 3.806291103363037,\n",
       " 3.5367274284362793,\n",
       " 5.0032148361206055,\n",
       " 3.246645927429199,\n",
       " 3.615021228790283,\n",
       " 3.6085116863250732,\n",
       " 3.576483726501465,\n",
       " 4.659597396850586,\n",
       " 3.0176503658294678,\n",
       " 3.2810754776000977,\n",
       " 3.3607065677642822,\n",
       " 3.031536817550659,\n",
       " 4.48862361907959,\n",
       " 4.069008827209473,\n",
       " 4.269596576690674,\n",
       " 2.6211605072021484,\n",
       " 4.047004699707031,\n",
       " 2.3188986778259277,\n",
       " 2.62353777885437,\n",
       " 2.9621944427490234,\n",
       " 2.5069777965545654,\n",
       " 2.727135181427002,\n",
       " 4.88741397857666,\n",
       " 4.026318550109863,\n",
       " 3.5220582485198975,\n",
       " 2.3996293544769287,\n",
       " 1.9940085411071777,\n",
       " 1.9122713804244995,\n",
       " 4.229783058166504,\n",
       " 4.256388187408447,\n",
       " 3.1194345951080322,\n",
       " 4.226412296295166,\n",
       " 2.8547260761260986,\n",
       " 2.707493305206299,\n",
       " 3.162029981613159,\n",
       " 2.9248971939086914,\n",
       " 3.687192678451538,\n",
       " 4.469163417816162,\n",
       " 2.8963255882263184,\n",
       " 3.7827911376953125,\n",
       " 3.362586498260498,\n",
       " 2.7897634506225586,\n",
       " 4.611191272735596,\n",
       " 4.30178165435791,\n",
       " 4.219703674316406,\n",
       " 3.705806016921997,\n",
       " 4.789865493774414,\n",
       " 2.543447732925415,\n",
       " 3.7314772605895996,\n",
       " 4.078619003295898,\n",
       " 4.372767925262451,\n",
       " 4.094022274017334,\n",
       " 3.8680460453033447,\n",
       " 2.858624219894409,\n",
       " 3.9701220989227295,\n",
       " 3.2039449214935303,\n",
       " 3.771613597869873,\n",
       " 3.2287867069244385,\n",
       " 3.1280438899993896,\n",
       " 4.158901214599609,\n",
       " 4.857572078704834,\n",
       " 3.9292354583740234,\n",
       " 4.757476329803467,\n",
       " 4.181635856628418,\n",
       " 3.948038101196289,\n",
       " 4.213578224182129,\n",
       " 3.6718521118164062,\n",
       " 3.7218081951141357,\n",
       " 4.783576011657715,\n",
       " 3.4181692600250244,\n",
       " 4.783320903778076,\n",
       " 4.265982151031494,\n",
       " 4.419861316680908,\n",
       " 3.6849803924560547,\n",
       " 4.331888198852539,\n",
       " 4.524782180786133,\n",
       " 3.6686694622039795,\n",
       " 4.0550150871276855,\n",
       " 3.8686771392822266,\n",
       " 4.05309534072876,\n",
       " 3.6957054138183594,\n",
       " 4.06046199798584,\n",
       " 3.7660512924194336,\n",
       " 3.5217125415802,\n",
       " 3.4073076248168945,\n",
       " 2.3179707527160645,\n",
       " 2.922131299972534,\n",
       " 2.4439802169799805,\n",
       " 3.403581142425537,\n",
       " 4.616495609283447,\n",
       " 3.7983274459838867,\n",
       " 3.6028902530670166,\n",
       " 2.854210615158081,\n",
       " 4.371085166931152,\n",
       " 3.542125940322876,\n",
       " 3.619292974472046,\n",
       " 3.502340316772461,\n",
       " 2.854095697402954,\n",
       " 3.1314351558685303,\n",
       " 2.4028983116149902,\n",
       " 3.2958943843841553,\n",
       " 4.760965347290039,\n",
       " 3.364124298095703,\n",
       " 3.264681816101074,\n",
       " 3.432138442993164,\n",
       " 3.5224545001983643,\n",
       " 2.717822313308716,\n",
       " 3.098358392715454,\n",
       " 3.7589545249938965,\n",
       " 3.3083882331848145,\n",
       " 3.2879271507263184,\n",
       " 4.83039665222168,\n",
       " 3.554762840270996,\n",
       " 4.238330364227295,\n",
       " 2.894824504852295,\n",
       " 3.501734733581543,\n",
       " 3.632664918899536,\n",
       " 3.3646528720855713,\n",
       " 3.330350637435913,\n",
       " 4.691900730133057,\n",
       " 4.1114983558654785,\n",
       " 3.1584057807922363,\n",
       " 2.656547784805298,\n",
       " 3.2130229473114014,\n",
       " 3.7914373874664307,\n",
       " 4.3029913902282715,\n",
       " 3.677142858505249,\n",
       " 3.5981712341308594,\n",
       " 3.862903594970703,\n",
       " 3.1340224742889404,\n",
       " 3.525339126586914,\n",
       " 4.283507823944092,\n",
       " 4.9759063720703125,\n",
       " 4.374526023864746,\n",
       " 2.9633448123931885,\n",
       " 2.8155527114868164,\n",
       " 2.9552924633026123,\n",
       " 4.506672382354736,\n",
       " 4.412337303161621,\n",
       " 2.378675937652588,\n",
       " 2.435838460922241,\n",
       " 3.786817789077759,\n",
       " 3.75998854637146,\n",
       " 4.839788436889648,\n",
       " 2.8119521141052246,\n",
       " 3.5531182289123535,\n",
       " 4.293762683868408,\n",
       " 3.536140203475952,\n",
       " 3.0806896686553955,\n",
       " 3.1453464031219482,\n",
       " 3.5777719020843506,\n",
       " 3.520531177520752,\n",
       " 2.8586690425872803,\n",
       " 3.6032118797302246,\n",
       " 5.025937557220459,\n",
       " 4.198695659637451,\n",
       " 4.284770965576172,\n",
       " 3.718045711517334,\n",
       " 3.7456142902374268,\n",
       " 3.4839909076690674,\n",
       " 3.5295839309692383,\n",
       " 4.627419948577881,\n",
       " 4.724029541015625,\n",
       " 3.7747092247009277,\n",
       " 3.6090147495269775,\n",
       " 2.9333670139312744,\n",
       " 3.1337316036224365,\n",
       " 5.066699504852295,\n",
       " 3.2738499641418457,\n",
       " 2.7545549869537354,\n",
       " 3.25762677192688,\n",
       " 2.2172493934631348,\n",
       " 2.8452353477478027,\n",
       " 4.859141826629639,\n",
       " 4.211518287658691,\n",
       " 4.461864471435547,\n",
       " 4.837075233459473,\n",
       " 2.8334972858428955,\n",
       " 3.0903704166412354,\n",
       " 3.95564603805542,\n",
       " 4.124110221862793,\n",
       " 3.6091935634613037,\n",
       " 3.5020623207092285,\n",
       " 3.4889538288116455,\n",
       " 3.5450854301452637,\n",
       " 4.966812610626221,\n",
       " 4.4494805335998535,\n",
       " 5.044631481170654,\n",
       " 4.024578094482422,\n",
       " 4.915173053741455,\n",
       " 3.833031177520752,\n",
       " 4.517342567443848,\n",
       " 4.313720226287842,\n",
       " 3.850721597671509,\n",
       " 3.8155038356781006,\n",
       " 4.112907886505127,\n",
       " 4.86217737197876,\n",
       " 4.333601951599121,\n",
       " 4.73466157913208,\n",
       " 3.310356616973877,\n",
       " 3.5477547645568848,\n",
       " 3.265381097793579,\n",
       " 3.171015739440918,\n",
       " 3.2912418842315674,\n",
       " 4.192261695861816,\n",
       " 4.0839524269104,\n",
       " 4.255380153656006,\n",
       " 3.773644208908081,\n",
       " 3.5293915271759033,\n",
       " 4.011842250823975,\n",
       " 4.3599467277526855,\n",
       " 4.40781831741333,\n",
       " 3.768886089324951,\n",
       " 3.401554822921753,\n",
       " 3.5672085285186768,\n",
       " 4.487469673156738,\n",
       " 3.984740972518921,\n",
       " 3.810853958129883,\n",
       " 3.410945177078247,\n",
       " 3.3572158813476562,\n",
       " 2.9415652751922607,\n",
       " 3.620220422744751,\n",
       " 5.038943767547607,\n",
       " 4.158088684082031,\n",
       " 4.312097549438477,\n",
       " 3.580256938934326,\n",
       " 3.208766222000122,\n",
       " 3.5754191875457764,\n",
       " 3.439378499984741,\n",
       " 4.9618144035339355,\n",
       " 4.325603008270264,\n",
       " 3.4722235202789307,\n",
       " 3.735635995864868,\n",
       " 3.1994292736053467,\n",
       " 4.073024272918701,\n",
       " 3.8811371326446533,\n",
       " 3.269711971282959,\n",
       " 3.56235933303833,\n",
       " 3.579249620437622,\n",
       " 3.5494978427886963,\n",
       " 4.2921671867370605,\n",
       " 4.155022621154785,\n",
       " 3.154374361038208,\n",
       " 3.9999403953552246,\n",
       " 3.480508804321289,\n",
       " 4.143824100494385,\n",
       " 3.781379461288452,\n",
       " 3.3784894943237305,\n",
       " 2.7689874172210693,\n",
       " 2.778425455093384,\n",
       " 4.6787896156311035,\n",
       " 3.644577980041504,\n",
       " 2.967442750930786,\n",
       " 4.30281400680542,\n",
       " 3.931356191635132,\n",
       " 3.7979321479797363,\n",
       " 4.394096374511719,\n",
       " 3.905461072921753,\n",
       " 4.204758644104004,\n",
       " 2.188080310821533,\n",
       " 2.4953815937042236,\n",
       " 3.0464863777160645,\n",
       " 4.666144371032715,\n",
       " 3.1577341556549072,\n",
       " 3.864222764968872,\n",
       " 4.408757209777832,\n",
       " 3.59543514251709,\n",
       " 2.6260058879852295,\n",
       " 2.5487828254699707,\n",
       " 2.766925811767578,\n",
       " 4.335907936096191,\n",
       " 4.697760105133057,\n",
       " 3.6664798259735107,\n",
       " 2.1263480186462402,\n",
       " 2.838888645172119,\n",
       " 2.1814675331115723,\n",
       " 3.5639634132385254,\n",
       " 4.429337978363037,\n",
       " 3.9712417125701904,\n",
       " 3.310105085372925,\n",
       " 2.679685115814209,\n",
       " 3.464465856552124,\n",
       " 3.160764694213867,\n",
       " 3.3420400619506836,\n",
       " 4.551608085632324,\n",
       " 4.588009357452393,\n",
       " 3.8914031982421875,\n",
       " 2.6537070274353027,\n",
       " 3.0782127380371094,\n",
       " 2.6831212043762207,\n",
       " 4.453522205352783,\n",
       " 3.4631543159484863,\n",
       " 4.62030029296875,\n",
       " 3.0052847862243652,\n",
       " 4.5078911781311035,\n",
       " 3.4313693046569824,\n",
       " 3.4628946781158447,\n",
       " 4.3729567527771,\n",
       " 4.813615322113037,\n",
       " 2.8810081481933594,\n",
       " 3.4837067127227783,\n",
       " 3.3473095893859863,\n",
       " 4.484167575836182,\n",
       " 3.649451494216919,\n",
       " 3.7220237255096436,\n",
       " 3.6787660121917725,\n",
       " 3.179331064224243,\n",
       " 4.32280969619751,\n",
       " 4.207442760467529,\n",
       " 3.9921014308929443,\n",
       " 4.3504638671875,\n",
       " 4.037327766418457,\n",
       " 4.285089492797852,\n",
       " 4.694761276245117,\n",
       " 4.596168041229248,\n",
       " 4.339396953582764,\n",
       " 2.9943623542785645,\n",
       " 2.501927137374878,\n",
       " 3.7284181118011475,\n",
       " 4.444700241088867,\n",
       " 3.860398292541504,\n",
       " 3.277371406555176,\n",
       " 4.92255973815918,\n",
       " 3.923245429992676,\n",
       " 3.708484411239624,\n",
       " 4.339272975921631,\n",
       " 4.909685134887695,\n",
       " 4.7068071365356445,\n",
       " 3.5659244060516357,\n",
       " 4.0977277755737305,\n",
       " 3.8250889778137207,\n",
       " 3.4420413970947266,\n",
       " 3.816314935684204,\n",
       " 4.334386348724365,\n",
       " 4.042040824890137,\n",
       " 4.063299179077148,\n",
       " 3.9540445804595947,\n",
       " 4.253570556640625,\n",
       " 4.030445575714111,\n",
       " 4.405829906463623,\n",
       " 4.113144874572754,\n",
       " 4.004637241363525,\n",
       " 3.9684181213378906,\n",
       " 3.5503740310668945,\n",
       " 4.067720890045166,\n",
       " 3.4044389724731445,\n",
       " 4.6365580558776855,\n",
       " 4.29210090637207,\n",
       " 2.941354274749756,\n",
       " 3.0200209617614746,\n",
       " 2.9821014404296875,\n",
       " 4.600832939147949,\n",
       " 4.659693717956543,\n",
       " 1.8279577493667603,\n",
       " 1.8383151292800903,\n",
       " 2.0038676261901855,\n",
       " 3.5333375930786133,\n",
       " 4.936089038848877,\n",
       " 4.907453536987305,\n",
       " 2.8539600372314453,\n",
       " 2.7923073768615723,\n",
       " 2.9031624794006348,\n",
       " 4.689105987548828,\n",
       " 3.4691243171691895,\n",
       " 3.8818886280059814,\n",
       " 3.1515583992004395,\n",
       " 3.9627649784088135,\n",
       " 2.9806225299835205,\n",
       " 3.050192356109619,\n",
       " 3.8244729042053223,\n",
       " 4.749527931213379,\n",
       " 3.5203914642333984,\n",
       " 3.45108962059021,\n",
       " 3.192437171936035,\n",
       " 5.090250492095947,\n",
       " 3.8096585273742676,\n",
       " 4.770778179168701,\n",
       " 3.560027837753296,\n",
       " 3.478846311569214,\n",
       " 3.65163516998291,\n",
       " 3.919663906097412,\n",
       " 2.3276655673980713,\n",
       " 2.7874701023101807,\n",
       " 4.367776870727539,\n",
       " 3.8104512691497803,\n",
       " 4.334634780883789,\n",
       " 3.7961201667785645,\n",
       " 3.725801944732666,\n",
       " 3.254486083984375,\n",
       " 3.5007126331329346,\n",
       " 3.7583279609680176,\n",
       " 4.502964496612549,\n",
       " 3.307382583618164,\n",
       " 4.990978240966797,\n",
       " 4.094486713409424,\n",
       " 3.3847460746765137,\n",
       " 3.6751625537872314,\n",
       " 4.133006572723389,\n",
       " 4.019586086273193,\n",
       " 4.697091579437256,\n",
       " 4.136460304260254,\n",
       " 4.121946334838867,\n",
       " 4.582435607910156,\n",
       " 1.4986549615859985,\n",
       " 1.4838337898254395,\n",
       " 1.049795389175415,\n",
       " 4.922146797180176,\n",
       " 3.034489870071411,\n",
       " 3.856739044189453,\n",
       " 1.2551478147506714,\n",
       " 0.4310820996761322,\n",
       " 1.2262574434280396,\n",
       " 4.60775899887085,\n",
       " 4.554632663726807,\n",
       " 3.52282452583313,\n",
       " 4.379169464111328,\n",
       " 3.4255666732788086,\n",
       " 3.311511993408203,\n",
       " 4.73715353012085,\n",
       " 4.159553050994873,\n",
       " 4.393411159515381,\n",
       " 3.521162986755371,\n",
       " 4.3907575607299805,\n",
       " 4.335107803344727,\n",
       " 3.896491765975952,\n",
       " 4.425217628479004,\n",
       " 3.567077159881592,\n",
       " 1.1690658330917358,\n",
       " 1.1286228895187378,\n",
       " 0.8981605768203735,\n",
       " 3.0940115451812744,\n",
       " 4.1285600662231445,\n",
       " 3.769808292388916,\n",
       " 3.978813886642456,\n",
       " 3.500335693359375,\n",
       " 3.7534992694854736,\n",
       " 4.487242698669434,\n",
       " 4.386911869049072,\n",
       " 3.681863307952881,\n",
       " 3.6190178394317627,\n",
       " 4.880001068115234,\n",
       " 4.599410533905029,\n",
       " 2.994100570678711,\n",
       " 4.27538537979126,\n",
       " 3.0240375995635986,\n",
       " 3.8724844455718994,\n",
       " 3.7269279956817627,\n",
       " 2.0730783939361572,\n",
       " 1.5638108253479004,\n",
       " 4.298613548278809,\n",
       " 4.1426777839660645,\n",
       " 4.035708427429199,\n",
       " 3.994117498397827,\n",
       " 3.6808948516845703,\n",
       " 4.282259464263916,\n",
       " 3.836272716522217,\n",
       " 4.200143814086914,\n",
       " 3.691664218902588,\n",
       " 4.237315654754639,\n",
       " 4.365403652191162,\n",
       " 3.470893383026123,\n",
       " 3.219597101211548,\n",
       " 2.7320139408111572,\n",
       " 3.3798422813415527,\n",
       " 4.470202445983887,\n",
       " 3.0552141666412354,\n",
       " 3.11207914352417,\n",
       " 4.602987289428711,\n",
       " 4.822460651397705,\n",
       " 3.53645396232605,\n",
       " 4.6807050704956055,\n",
       " 2.6067066192626953,\n",
       " 2.6560001373291016,\n",
       " 4.15919828414917,\n",
       " 3.835083484649658,\n",
       " 4.524862289428711,\n",
       " 3.937570810317993,\n",
       " 3.711493968963623,\n",
       " 3.4988558292388916,\n",
       " 2.404343605041504,\n",
       " 2.9751338958740234,\n",
       " 3.56294846534729,\n",
       " 4.305230140686035,\n",
       " 3.7559218406677246,\n",
       " 2.784675121307373,\n",
       " 2.9264988899230957,\n",
       " 2.139662981033325,\n",
       " 4.4746880531311035,\n",
       " 3.6725666522979736,\n",
       " 4.384108543395996,\n",
       " 3.1958630084991455,\n",
       " 4.694231986999512,\n",
       " 3.1645939350128174,\n",
       " 3.895250082015991,\n",
       " 3.2877745628356934,\n",
       " 3.612811803817749,\n",
       " 1.665520429611206,\n",
       " 3.2356536388397217,\n",
       " 2.1332058906555176,\n",
       " 3.528812885284424,\n",
       " 3.2262580394744873,\n",
       " 2.4373364448547363,\n",
       " 3.410048007965088,\n",
       " 4.8005242347717285,\n",
       " 4.206958293914795,\n",
       " 4.046787738800049,\n",
       " 4.401603698730469,\n",
       " 4.274663925170898,\n",
       " 4.148319721221924,\n",
       " 3.7272701263427734,\n",
       " 2.3009753227233887,\n",
       " 3.459484338760376,\n",
       " 1.7949771881103516,\n",
       " 2.0618669986724854,\n",
       " 3.8747990131378174,\n",
       " 3.7713749408721924,\n",
       " 4.0746636390686035,\n",
       " 1.8117218017578125,\n",
       " 0.6157393455505371,\n",
       " 0.7831518054008484,\n",
       " 0.7351555228233337,\n",
       " 4.871920585632324,\n",
       " 2.83209228515625,\n",
       " 4.8233537673950195,\n",
       " 4.0890045166015625,\n",
       " 2.8580074310302734,\n",
       " 4.195767402648926,\n",
       " 3.6598119735717773,\n",
       " 4.648269176483154,\n",
       " 3.4951343536376953,\n",
       " 3.5639514923095703,\n",
       " 4.0631513595581055,\n",
       " 3.178274631500244,\n",
       " 4.250201225280762,\n",
       " 4.236236095428467,\n",
       " 3.7672858238220215,\n",
       " 3.8181514739990234,\n",
       " 4.488842010498047,\n",
       " 4.198197841644287,\n",
       " 4.433931827545166,\n",
       " 4.118340969085693,\n",
       " 3.9666435718536377,\n",
       " 4.408435344696045,\n",
       " 4.380223274230957,\n",
       " 4.412100791931152,\n",
       " 3.2612874507904053,\n",
       " 4.305574893951416,\n",
       " 4.136183738708496,\n",
       " 4.5440449714660645,\n",
       " 4.504227161407471,\n",
       " 3.439689874649048,\n",
       " 3.651958703994751,\n",
       " 4.827430248260498,\n",
       " 3.6399343013763428,\n",
       " 3.688516139984131,\n",
       " 3.657139539718628,\n",
       " 0.8116456270217896,\n",
       " 0.7205803990364075,\n",
       " 0.7081038355827332,\n",
       " 4.6846089363098145,\n",
       " 3.3445518016815186,\n",
       " 3.46576189994812,\n",
       " 3.597235918045044,\n",
       " 3.3657448291778564,\n",
       " 3.629589319229126,\n",
       " 3.540121078491211,\n",
       " 1.751696228981018,\n",
       " 3.8213319778442383,\n",
       " 4.537679672241211,\n",
       " 3.8713438510894775,\n",
       " 4.620936393737793,\n",
       " 4.02324914932251,\n",
       " 4.527559280395508,\n",
       " 3.764162302017212,\n",
       " 1.4846776723861694,\n",
       " 0.9440876841545105,\n",
       " 1.3693041801452637,\n",
       " 4.568261623382568,\n",
       " 3.7150285243988037,\n",
       " 3.2482821941375732,\n",
       " 3.0654892921447754,\n",
       " 3.5289299488067627,\n",
       " 3.6783528327941895,\n",
       " 3.4109976291656494,\n",
       " 3.9781081676483154,\n",
       " 3.0888049602508545,\n",
       " 2.327183246612549,\n",
       " 4.057088375091553,\n",
       " 4.913893222808838,\n",
       " 3.769882917404175,\n",
       " 1.140521764755249,\n",
       " 2.1763298511505127,\n",
       " 1.06157648563385,\n",
       " 4.437477111816406,\n",
       " 3.1425940990448,\n",
       " 3.4107909202575684,\n",
       " 2.370863914489746,\n",
       " 1.531798005104065,\n",
       " 4.783616542816162,\n",
       " 1.9697734117507935,\n",
       " 3.0059361457824707,\n",
       " 4.043191432952881,\n",
       " 4.879519462585449,\n",
       " 3.2499516010284424,\n",
       " 3.4282946586608887,\n",
       " 3.270326852798462,\n",
       " 3.3082146644592285,\n",
       " 3.7069432735443115,\n",
       " 3.9017257690429688,\n",
       " 4.534026622772217,\n",
       " 3.91756272315979,\n",
       " 4.103597640991211,\n",
       " 3.5505573749542236,\n",
       " 3.896758556365967,\n",
       " 3.217714309692383,\n",
       " 3.008017063140869,\n",
       " 1.4043911695480347,\n",
       " 1.370956540107727,\n",
       " 1.189504623413086,\n",
       " 4.144554138183594,\n",
       " 4.173229217529297,\n",
       " 3.4361298084259033,\n",
       " 3.34131121635437,\n",
       " 4.147546291351318,\n",
       " 3.6061949729919434,\n",
       " 1.6197121143341064,\n",
       " 1.2282800674438477,\n",
       " 3.6022756099700928,\n",
       " 4.510534763336182,\n",
       " 2.907621145248413,\n",
       " 2.244749069213867,\n",
       " 2.5321056842803955,\n",
       " 2.6896538734436035,\n",
       " 3.413494110107422,\n",
       " 3.616788864135742,\n",
       " 4.10571813583374,\n",
       " 4.281618595123291,\n",
       " 4.764091491699219,\n",
       " 4.2022294998168945,\n",
       " 3.630337715148926,\n",
       " 4.147028923034668,\n",
       " 4.3558526039123535,\n",
       " 5.31894588470459,\n",
       " 4.213830947875977,\n",
       " 4.316459655761719,\n",
       " 2.592458963394165,\n",
       " 3.0372226238250732,\n",
       " 4.363647937774658,\n",
       " 3.331019401550293,\n",
       " 3.3874447345733643,\n",
       " 3.224353551864624,\n",
       " 4.500816345214844,\n",
       " 3.3987698554992676,\n",
       " 4.250217914581299,\n",
       " 3.3571577072143555,\n",
       " 5.1002678871154785,\n",
       " 3.495579242706299,\n",
       " 4.6190361976623535,\n",
       " 4.612505912780762,\n",
       " 4.314193248748779,\n",
       " 3.634343147277832,\n",
       " 4.311694622039795,\n",
       " 3.4309136867523193,\n",
       " 3.1714727878570557,\n",
       " 3.47309947013855,\n",
       " 4.46330451965332,\n",
       " 4.694082260131836,\n",
       " 4.473329067230225,\n",
       " 1.5706915855407715,\n",
       " 1.5094327926635742,\n",
       " 1.5192937850952148,\n",
       " 4.581990718841553,\n",
       " 3.833859920501709,\n",
       " 3.7078535556793213,\n",
       " 3.137897253036499,\n",
       " 3.997584342956543,\n",
       " 1.7993744611740112,\n",
       " 2.909637212753296,\n",
       " 3.6875336170196533,\n",
       " 4.65724515914917,\n",
       " 3.718600273132324,\n",
       " 4.851183891296387,\n",
       " 5.062201499938965,\n",
       " 3.8414206504821777,\n",
       " 4.395733833312988,\n",
       " 2.7603256702423096,\n",
       " 4.765191078186035,\n",
       " 4.9352288246154785,\n",
       " 0.6543271541595459,\n",
       " 2.2021067142486572,\n",
       " 3.34503173828125,\n",
       " 3.610954523086548,\n",
       " 3.5632741451263428,\n",
       " 4.424434661865234,\n",
       " 3.606194019317627,\n",
       " 2.722574234008789,\n",
       " 4.3955793380737305,\n",
       " 3.552274465560913,\n",
       " 4.772580623626709,\n",
       " 4.292840480804443,\n",
       " 4.5953288078308105,\n",
       " 4.1752166748046875,\n",
       " 4.347461700439453,\n",
       " 4.80361795425415,\n",
       " 4.444317817687988,\n",
       " 2.328517436981201,\n",
       " 4.541893482208252,\n",
       " 1.69277822971344,\n",
       " 1.0910638570785522,\n",
       " 3.7677717208862305,\n",
       " 4.964366912841797,\n",
       " 4.1553826332092285,\n",
       " 4.116229057312012,\n",
       " 5.013660430908203,\n",
       " 4.251735687255859,\n",
       " 3.3534255027770996,\n",
       " 4.835902690887451,\n",
       " 2.356981039047241,\n",
       " 1.4138883352279663,\n",
       " 1.6126658916473389,\n",
       " 1.1750694513320923,\n",
       " 3.2955641746520996,\n",
       " 3.3544278144836426,\n",
       " 3.1407783031463623,\n",
       " 1.0311005115509033,\n",
       " 1.0846247673034668,\n",
       " 0.7481649518013,\n",
       " 0.8305424451828003,\n",
       " 4.4407782554626465,\n",
       " 3.6034157276153564,\n",
       " 4.702674388885498,\n",
       " 4.221001625061035,\n",
       " 4.551331043243408,\n",
       " 4.355895042419434,\n",
       " 4.143651485443115,\n",
       " 4.469150543212891,\n",
       " 3.844358205795288,\n",
       " 4.214605331420898,\n",
       " 4.670539379119873,\n",
       " 4.408425331115723,\n",
       " 2.920025587081909,\n",
       " 5.176651954650879,\n",
       " 4.2864227294921875,\n",
       " 4.513238430023193,\n",
       " 3.6292316913604736,\n",
       " 4.518442153930664,\n",
       " 3.5915327072143555,\n",
       " 3.065614700317383,\n",
       " 4.5323872566223145,\n",
       " 4.618137359619141,\n",
       " 3.955383777618408,\n",
       " 4.087532043457031,\n",
       " 4.362137317657471,\n",
       " 3.411699056625366,\n",
       " 2.7202298641204834,\n",
       " 1.656488060951233,\n",
       " 4.1190009117126465,\n",
       " 3.3203649520874023,\n",
       " 1.8672362565994263,\n",
       " 3.425201654434204,\n",
       " 3.7460219860076904,\n",
       " 4.483539581298828,\n",
       " 4.857556343078613,\n",
       " 3.33339524269104,\n",
       " 4.057581424713135,\n",
       " 3.9464735984802246,\n",
       " 2.591176748275757,\n",
       " 3.29555344581604,\n",
       " 4.3598313331604,\n",
       " 4.874789714813232,\n",
       " 3.3862457275390625,\n",
       " 4.289552211761475,\n",
       " 3.3627772331237793,\n",
       " 3.271530866622925,\n",
       " 4.914224147796631,\n",
       " 3.850327730178833,\n",
       " 4.018847942352295,\n",
       " 4.226015090942383,\n",
       " 2.2647910118103027,\n",
       " 2.250195264816284,\n",
       " 2.7321040630340576,\n",
       " 2.8746633529663086,\n",
       " 2.9031546115875244,\n",
       " 2.7263078689575195,\n",
       " 3.605267286300659,\n",
       " 0.9890152812004089,\n",
       " 1.4059171676635742,\n",
       " 4.881989002227783,\n",
       " 2.9108827114105225,\n",
       " 3.2351489067077637,\n",
       " 3.6655285358428955,\n",
       " 3.3207931518554688,\n",
       " 4.788557529449463,\n",
       " 3.5773253440856934,\n",
       " 5.074417591094971,\n",
       " 3.384688377380371,\n",
       " 4.1335649490356445,\n",
       " 3.9302468299865723,\n",
       " 3.498871326446533,\n",
       " 3.876851797103882,\n",
       " 3.4393529891967773,\n",
       " 3.9908995628356934,\n",
       " 4.914005756378174,\n",
       " 4.664135932922363,\n",
       " 3.709599494934082,\n",
       " 4.376264572143555,\n",
       " 5.1097588539123535,\n",
       " 3.6886160373687744,\n",
       " 4.083962917327881,\n",
       " 4.3525776863098145,\n",
       " 3.6794066429138184,\n",
       " 3.999696731567383,\n",
       " 4.14191198348999,\n",
       " 4.123342514038086,\n",
       " 3.814800262451172,\n",
       " 2.992064952850342,\n",
       " 0.9719491004943848,\n",
       " 2.5447633266448975,\n",
       " 1.0242840051651,\n",
       " 3.2662594318389893,\n",
       " 3.795464277267456,\n",
       " 4.132190227508545,\n",
       " 1.317265272140503,\n",
       " 0.8772419691085815,\n",
       " 1.1208170652389526,\n",
       " 2.0726239681243896,\n",
       " 5.022651195526123,\n",
       " 3.761695384979248,\n",
       " 3.6089260578155518,\n",
       " 4.695664405822754,\n",
       " 3.8147261142730713,\n",
       " 2.2805593013763428,\n",
       " 1.0511587858200073,\n",
       " 2.596893072128296,\n",
       " 1.0241131782531738,\n",
       " 1.5758531093597412,\n",
       " 1.111573576927185,\n",
       " 3.6429295539855957,\n",
       " 3.9057843685150146,\n",
       " 0.6590427160263062,\n",
       " 3.996188163757324,\n",
       " 3.2970669269561768,\n",
       " 1.7359267473220825,\n",
       " 1.7542445659637451,\n",
       " 4.09210729598999,\n",
       " 4.718498706817627,\n",
       " 3.7161362171173096,\n",
       " 1.1547490358352661,\n",
       " ...]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8278104120877068, 0.0)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "stats.pearsonr(test1, np.array(test['relatedness_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>entailment_judgment</th>\n",
       "      <th>entailment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>there is no boy playing outdoors and there is no man smiling</td>\n",
       "      <td>a group of kids is playing in a yard and an old man is standing in the background</td>\n",
       "      <td>3.300</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>a group of boys in a yard is playing and a man is standing in the background</td>\n",
       "      <td>the young boys are playing outdoors and the man is smiling nearby</td>\n",
       "      <td>3.700</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>a group of children is playing in the house and there is no man standing in the background</td>\n",
       "      <td>the young boys are playing outdoors and the man is smiling nearby</td>\n",
       "      <td>3.000</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>a brown dog is attacking another animal in front of the tall man in pants</td>\n",
       "      <td>a brown dog is attacking another animal in front of the man in pants</td>\n",
       "      <td>4.900</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>a brown dog is attacking another animal in front of the man in pants</td>\n",
       "      <td>a brown dog is helping another animal in front of the man in pants</td>\n",
       "      <td>3.665</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4922</td>\n",
       "      <td>9991</td>\n",
       "      <td>the young girl is blowing a bubble that is huge</td>\n",
       "      <td>there is no girl in pink twirling a ribbon</td>\n",
       "      <td>2.100</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4923</td>\n",
       "      <td>9992</td>\n",
       "      <td>a dog in a colored coat is running across the yard</td>\n",
       "      <td>the flute is being played by one man</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4924</td>\n",
       "      <td>9994</td>\n",
       "      <td>a boy is happily playing the piano</td>\n",
       "      <td>a white bird is landing swiftly in the water</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4925</td>\n",
       "      <td>9995</td>\n",
       "      <td>the girl  who is little  is combing her hair into a pony tail</td>\n",
       "      <td>two people wearing helmets are driving over the yellow and white flowers</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4926</td>\n",
       "      <td>9996</td>\n",
       "      <td>a man is in a parking lot and is playing tennis against a large wall</td>\n",
       "      <td>the snowboarder is leaping fearlessly over white snow</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4927 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pair_ID  \\\n",
       "0     6         \n",
       "1     7         \n",
       "2     8         \n",
       "3     10        \n",
       "4     11        \n",
       "...   ..        \n",
       "4922  9991      \n",
       "4923  9992      \n",
       "4924  9994      \n",
       "4925  9995      \n",
       "4926  9996      \n",
       "\n",
       "                                                                                      sentence_A  \\\n",
       "0     there is no boy playing outdoors and there is no man smiling                                 \n",
       "1     a group of boys in a yard is playing and a man is standing in the background                 \n",
       "2     a group of children is playing in the house and there is no man standing in the background   \n",
       "3     a brown dog is attacking another animal in front of the tall man in pants                    \n",
       "4     a brown dog is attacking another animal in front of the man in pants                         \n",
       "...                                                                    ...                         \n",
       "4922  the young girl is blowing a bubble that is huge                                              \n",
       "4923  a dog in a colored coat is running across the yard                                           \n",
       "4924  a boy is happily playing the piano                                                           \n",
       "4925  the girl  who is little  is combing her hair into a pony tail                                \n",
       "4926  a man is in a parking lot and is playing tennis against a large wall                         \n",
       "\n",
       "                                                                             sentence_B  \\\n",
       "0     a group of kids is playing in a yard and an old man is standing in the background   \n",
       "1     the young boys are playing outdoors and the man is smiling nearby                   \n",
       "2     the young boys are playing outdoors and the man is smiling nearby                   \n",
       "3     a brown dog is attacking another animal in front of the man in pants                \n",
       "4     a brown dog is helping another animal in front of the man in pants                  \n",
       "...                                                                  ...                  \n",
       "4922  there is no girl in pink twirling a ribbon                                          \n",
       "4923  the flute is being played by one man                                                \n",
       "4924  a white bird is landing swiftly in the water                                        \n",
       "4925  two people wearing helmets are driving over the yellow and white flowers            \n",
       "4926  the snowboarder is leaping fearlessly over white snow                               \n",
       "\n",
       "      relatedness_score entailment_judgment  entailment_encoded  \n",
       "0     3.300              NEUTRAL             2                   \n",
       "1     3.700              NEUTRAL             2                   \n",
       "2     3.000              NEUTRAL             2                   \n",
       "3     4.900              ENTAILMENT          1                   \n",
       "4     3.665              NEUTRAL             2                   \n",
       "...     ...                  ...            ..                   \n",
       "4922  2.100              NEUTRAL             2                   \n",
       "4923  1.000              NEUTRAL             2                   \n",
       "4924  1.000              NEUTRAL             2                   \n",
       "4925  1.000              NEUTRAL             2                   \n",
       "4926  1.000              NEUTRAL             2                   \n",
       "\n",
       "[4927 rows x 6 columns]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.111167828875477"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(test1, test['relatedness_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in range(len(test['relatedness_score'])):\n",
    "    temp.append(abs(test['relatedness_score'][i] - test1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.915496826171875e-05"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "max_len = max_seq_length\n",
    "batch_size = 32\n",
    "embedding_dim = 300\n",
    "filters = 16\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(embedding_weights), embedding_dim,weights=[embedding_weights], \n",
    "                 input_length=max_seq_length, \n",
    "                 trainable=False ))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Conv1D(filters, kernel_size, padding = 'valid', activation = 'tanh'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters, kernel_size, padding = 'valid', activation = 'tanh'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_dims, activation = 'tanh'))\n",
    "model.add(Dropout(.1))\n",
    "\n",
    "left_input = Input(shape = (max_seq_length,))\n",
    "right_input = Input(shape = (max_seq_length,))\n",
    "left = model(left_input)\n",
    "right = model(right_input)\n",
    "\n",
    "l1_layer = Lambda(lambda x: K.abs(x[0] - x[1]))\n",
    "l1_distance = l1_layer([left, right])\n",
    "prediction = Dense(1, activation= 'selu')(l1_distance)\n",
    "siamese_cnn = Model([left_input, right_input], prediction)\n",
    "siamese_cnn.compile(loss = 'mse', optimizer = Adadelta())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "4500/4500 [==============================] - 2s 449us/step - loss: 2.7251 - val_loss: 13.6580\n",
      "Epoch 2/25\n",
      "4500/4500 [==============================] - 2s 381us/step - loss: 1.4848 - val_loss: 12.7637\n",
      "Epoch 3/25\n",
      "4500/4500 [==============================] - 2s 522us/step - loss: 1.3607 - val_loss: 12.8543\n",
      "Epoch 4/25\n",
      "4500/4500 [==============================] - 3s 717us/step - loss: 1.2845 - val_loss: 11.6533\n",
      "Epoch 5/25\n",
      "4500/4500 [==============================] - 3s 647us/step - loss: 1.2095 - val_loss: 9.1749\n",
      "Epoch 6/25\n",
      "4500/4500 [==============================] - 3s 599us/step - loss: 1.1472 - val_loss: 10.0103\n",
      "Epoch 7/25\n",
      "4500/4500 [==============================] - 3s 602us/step - loss: 1.0723 - val_loss: 9.2358\n",
      "Epoch 8/25\n",
      "4500/4500 [==============================] - 3s 643us/step - loss: 1.0390 - val_loss: 7.7876\n",
      "Epoch 9/25\n",
      "4500/4500 [==============================] - 3s 626us/step - loss: 0.9556 - val_loss: 7.3077\n",
      "Epoch 10/25\n",
      "4500/4500 [==============================] - 3s 637us/step - loss: 0.9129 - val_loss: 6.2839\n",
      "Epoch 11/25\n",
      "4500/4500 [==============================] - 3s 590us/step - loss: 0.8825 - val_loss: 7.3025\n",
      "Epoch 12/25\n",
      "4500/4500 [==============================] - 3s 598us/step - loss: 0.8343 - val_loss: 6.5004\n",
      "Epoch 13/25\n",
      "4500/4500 [==============================] - 3s 599us/step - loss: 0.8345 - val_loss: 6.5722\n",
      "Epoch 14/25\n",
      "4500/4500 [==============================] - 3s 599us/step - loss: 0.7859 - val_loss: 6.2910\n",
      "Epoch 15/25\n",
      "4500/4500 [==============================] - 3s 615us/step - loss: 0.7732 - val_loss: 4.6636\n",
      "Epoch 16/25\n",
      "4500/4500 [==============================] - 3s 598us/step - loss: 0.7480 - val_loss: 6.0288\n",
      "Epoch 17/25\n",
      "4500/4500 [==============================] - 3s 595us/step - loss: 0.7171 - val_loss: 5.1297\n",
      "Epoch 18/25\n",
      "4500/4500 [==============================] - 3s 598us/step - loss: 0.7010 - val_loss: 5.5760\n",
      "Epoch 19/25\n",
      "4500/4500 [==============================] - 3s 616us/step - loss: 0.6889 - val_loss: 5.2165\n",
      "Epoch 20/25\n",
      "4500/4500 [==============================] - 3s 603us/step - loss: 0.6844 - val_loss: 4.5097\n",
      "Epoch 21/25\n",
      "4500/4500 [==============================] - 3s 609us/step - loss: 0.6938 - val_loss: 4.7341\n",
      "Epoch 22/25\n",
      "4500/4500 [==============================] - 3s 603us/step - loss: 0.6468 - val_loss: 4.2954\n",
      "Epoch 23/25\n",
      "4500/4500 [==============================] - 3s 612us/step - loss: 0.6524 - val_loss: 4.0815\n",
      "Epoch 24/25\n",
      "4500/4500 [==============================] - 3s 609us/step - loss: 0.6235 - val_loss: 4.0329\n",
      "Epoch 25/25\n",
      "4500/4500 [==============================] - 3s 599us/step - loss: 0.6022 - val_loss: 3.8882\n"
     ]
    }
   ],
   "source": [
    "hist = siamese_cnn.fit([sent1_data, sent2_data], train['relatedness_score'], batch_size = 32, \n",
    "                 epochs = 25,\n",
    "                 validation_data = ([sent1_data_trial, sent2_data_trial], trial['relatedness_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = siamese_cnn.predict([sent1_data_test, sent2_data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for j in range(len(k)):\n",
    "    t.append(abs(k[j] - test['relatedness_score'][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8199873852354445, 0.0)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = []\n",
    "for j in range(len(k)):\n",
    "    t.append(abs(k[j] - test['relatedness_score'][j]))\n",
    "from scipy import stats\n",
    "stats.pearsonr(test1, np.array(test['relatedness_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.43151040130842"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(test1, test['relatedness_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Embedding, Dense, Input, Dropout, Reshape, BatchNormalization, TimeDistributed, Lambda, Layer, LSTM, Bidirectional, Convolution1D, GRU, add, concatenate\n",
    "from keras.callbacks import Callback, ModelCheckpoint, TensorBoard, BaseLogger, ReduceLROnPlateau\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Adagrad\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "def cosine_distance(y1, y2):\n",
    "    mult =  tf.multiply(y1, y2)\n",
    "    cosine_numerator = tf.reduce_sum( mult, axis = -1)\n",
    "    y1_norm = tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(y1), axis=-1 ), eps) ) \n",
    "    y2_norm = tf.sqrt(tf.maximum(tf.reduce_sum(tf.square(y2), axis=-1 ), eps) ) \n",
    "    return cosine_numerator / y1_norm / y2_norm\n",
    "\n",
    "def cal_relevancy_matrix(text_vector, hypo_vector):\n",
    "    text_vector_tmp = tf.expand_dims(text_vector, 1) # [batch_size, 1, question_len, dim]\n",
    "    hypo_vector_tmp = tf.expand_dims(hypo_vector, 2) # [batch_size, passage_len, 1, dim]\n",
    "    relevancy_matrix = cosine_distance(text_vector_tmp, hypo_vector_tmp) # [batch_size, passage_len, question_len]\n",
    "    return relevancy_matrix\n",
    "\n",
    "def mask_relevancy_matrix(relevancy_matrix, text_mask, hypo_mask):\n",
    "    relevancy_matrix = tf.multiply(relevancy_matrix, K.expand_dims(text_mask, 1))\n",
    "    relevancy_matrix = tf.multiply(relevancy_matrix, K.expand_dims(hypo_mask, 2))\n",
    "    return relevancy_matrix\n",
    "\n",
    "def max_mean_pooling(repres, cosine_matrix):\n",
    "    \n",
    "    repres.append(tf.reduce_max(cosine_matrix, axis = 2, keepdims = True))\n",
    "    repres.append(tf.reduce_mean(cosine_matrix, axis = 2, keepdims = True))\n",
    "\n",
    "    return repres\n",
    "\n",
    "def matching_layer(inputs):\n",
    "    forward_relevancy_matrix = cal_relevancy_matrix(inputs[0], inputs[2])\n",
    "    backward_relevancy_matrix = cal_relevancy_matrix(inputs[1], inputs[3])\n",
    "\n",
    "    representation = []\n",
    "\n",
    "    max_mean_pooling(representation, forward_relevancy_matrix)\n",
    "    max_mean_pooling(representation, backward_relevancy_matrix)\n",
    "    \n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchLayer(Layer):\n",
    "\n",
    "    def __init__(self, dim, seq_length, **kwargs):\n",
    "        super(MatchLayer, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.dim = dim\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        if not isinstance(input_shape, list):\n",
    "            raise ValueError('`MatchLayer` layer should be called '\n",
    "                             'on a list of inputs')\n",
    "        \n",
    "        if all([shape is None for shape in input_shape]):\n",
    "            return\n",
    "        \n",
    "        super(MatchLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if not isinstance(inputs, list):\n",
    "            raise ValueError('A `MatchLayer` layer should be called ')\n",
    "        \n",
    "        return matching_layer(inputs)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not isinstance(input_shape, list):\n",
    "            raise ValueError('A `MatchLayer` layer should be called '\n",
    "                             'on a list of inputs.')\n",
    "        \n",
    "        input_shapes = input_shape\n",
    "        output_shape = list(input_shapes[0])\n",
    "                             \n",
    "        return [ (None, output_shape[1] , 1) ] * 4 \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "\n",
    "        }\n",
    "        base_config = super(MatchLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "class MaxPoolingLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MaxPoolingLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(MaxPoolingLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return max_mean_pooling([], inputs)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):            \n",
    "        output_shape = list(input_shape)\n",
    "        return [ (None, output_shape[1] , 1) ] * 2\n",
    "    \n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return [mask, mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_len = len(embedding_weights)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {};\n",
    "def word_context(input, name):\n",
    "    embedding = Embedding(word_len + 1,\n",
    "                     embedding_dim,\n",
    "                     weights = [embedding_weights],\n",
    "                     input_length = max_seq_length,\n",
    "                     trainable = False,\n",
    "                      name = name + '_embedding')(input)\n",
    "    \n",
    "    word = Dropout(0.1)(embedding)\n",
    "\n",
    "    context = Bidirectional(LSTM(100, return_sequences = True),\n",
    "                            merge_mode = None,\n",
    "                            name = name + '_context')(word)\n",
    "    \n",
    "    return (word, context)\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    sentence1_input = Input(shape=(max_seq_length,), dtype='int32', name = 'text')\n",
    "    sentence2_input = Input(shape=(max_seq_length,), dtype='int32', name = 'hypothesis')\n",
    "    \n",
    "    (text_embedding, text_context) = word_context(sentence1_input, 'text')\n",
    "    (hypo_embedding, hypo_context) = word_context(sentence2_input, 'hypothesis')\n",
    "\n",
    "    left_context = []\n",
    "    left_context.extend(hypo_context)\n",
    "    left_context.extend(text_context)\n",
    "    \n",
    "    left_match = MatchLayer(embedding_dim, max_seq_length)( left_context )\n",
    "    \n",
    "    right_context = []\n",
    "    right_context.extend(text_context)\n",
    "    right_context.extend(hypo_context)\n",
    "    \n",
    "    right_match = MatchLayer(embedding_dim, max_seq_length)( right_context )\n",
    "    \n",
    "    cosine_left = Lambda(lambda x_input: cal_relevancy_matrix(x_input[0], x_input[1]))( [text_embedding, hypo_embedding] )\n",
    "    cosine_right = Lambda(lambda cosine: tf.transpose(cosine, perm=[0,2,1]))( cosine_left )\n",
    "    \n",
    "    left_representation = MaxPoolingLayer()( cosine_left )\n",
    "    right_representation = MaxPoolingLayer()( cosine_right )\n",
    "    \n",
    "    left_representation.extend( left_match )\n",
    "    right_representation.extend( right_match ) \n",
    "    \n",
    "    left = concatenate(left_representation, axis = 2)\n",
    "    left = Dropout(0.10)(left)\n",
    "    \n",
    "    right = concatenate(right_representation, axis = 2)\n",
    "    right = Dropout(0.10)(right)\n",
    "    \n",
    "\n",
    "    aggregation_left = Bidirectional(LSTM(100),\n",
    "                            name = 'aggregation_text_context')(left)\n",
    "    \n",
    "    aggregation_right = Bidirectional(LSTM(100),\n",
    "                            name = 'aggregation_hypo_context')(right)\n",
    "    \n",
    "    aggregation = concatenate([aggregation_left, aggregation_right], axis = -1)\n",
    "                               \n",
    "    #pred = Dense(200, activation = 'tanh', name = 'tanh_prediction')(aggregation)\n",
    "    pred = Dense(1, activation = 'selu', name = 'selu')(aggregation)\n",
    "    \n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    model = Model(inputs=[sentence1_input, sentence2_input], outputs = pred)\n",
    "    model.compile(loss = 'mse', \n",
    "              optimizer = optimizer\n",
    "              )\n",
    "    \n",
    "    print('Model created')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "models['adam'] = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sample_size = 1000 # Change this to -1 if you want all\n",
    "#max_seq_length = 28 # max 400\n",
    "\n",
    "model_name = 'small'\n",
    "\n",
    "lr = 0.001\n",
    "lr_decay = 1e-4\n",
    "epochs = 20\n",
    "batch_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start learning adam at 1584548726\n",
      "Epochs: 20\n",
      "Batch size: 15\n",
      "Train on 4050 samples, validate on 450 samples\n",
      "Epoch 1/10\n",
      " - 33s - loss: 0.0739 - val_loss: 0.9977\n",
      "Epoch 2/10\n",
      " - 38s - loss: 0.0745 - val_loss: 0.9441\n",
      "Epoch 3/10\n",
      " - 38s - loss: 0.0741 - val_loss: 0.9556\n",
      "Epoch 4/10\n",
      " - 38s - loss: 0.0707 - val_loss: 0.9462\n",
      "Epoch 5/10\n",
      " - 38s - loss: 0.0677 - val_loss: 0.9458\n",
      "Epoch 6/10\n",
      " - 40s - loss: 0.0623 - val_loss: 1.0383\n",
      "Epoch 7/10\n",
      " - 40s - loss: 0.0607 - val_loss: 1.0418\n",
      "Epoch 8/10\n",
      " - 40s - loss: 0.0604 - val_loss: 0.9578\n",
      "Epoch 9/10\n",
      " - 40s - loss: 0.0572 - val_loss: 0.9945\n",
      "Epoch 10/10\n",
      " - 39s - loss: 0.0594 - val_loss: 1.0143\n",
      "Time: 383\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name, model in models.items():\n",
    "    callbacks = [\n",
    "        BaseLogger(),\n",
    "        ReduceLROnPlateau(monitor = 'val_loss', factor=0.2, patience=5, min_lr=0.001),\n",
    "        TensorBoard(log_dir='./' + model_name + '-' + name + '-logs', histogram_freq=0, write_graph=True, write_images=True),\n",
    "        ModelCheckpoint(model_name + '-' + name + '-checkpoint-weights.{epoch:02d}-{val_acc:.2f}.hdf5', monitor='val_acc', save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    start_time = time()\n",
    "    print('')\n",
    "    print('Start learning %s at %d' % (name, start_time))\n",
    "    print('Epochs: %d' % epochs)\n",
    "    print('Batch size: %d' % batch_size)\n",
    "\n",
    "    history = model.fit([sent1_data, sent2_data],\n",
    "                        train['relatedness_score'],\n",
    "                        epochs = 10,\n",
    "                        batch_size = batch_size,\n",
    "                        validation_split = 0.1,\n",
    "                        shuffle = True, #True,\n",
    "                        verbose = 2,\n",
    "                        )\n",
    "\n",
    "    model.save(model_name + '-' + name + '-model.h5')\n",
    "    model.save_weights(model_name + '-' + name + '-weights.h5')\n",
    "\n",
    "    end_time = time()\n",
    "    average_time_per_epoch = (end_time - start_time) / epochs\n",
    "    results.append((history, average_time_per_epoch))\n",
    "    \n",
    "    print('Time: %d' % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = model.predict([sent1_data_test, sent2_data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = []\n",
    "for i in k.tolist():\n",
    "    test1.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6833786210570711, 0.0)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t = []\n",
    "# for j in range(len(k)):\n",
    "#     t.append(abs(k[j] - test['relatedness_score'][j]))\n",
    "from scipy import stats\n",
    "stats.pearsonr(test1, np.array(test['relatedness_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_t`rue, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.823850319496543"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(test1, test['relatedness_score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
