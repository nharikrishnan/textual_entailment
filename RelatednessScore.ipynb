{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import functions.utils\n",
    "from functions.utils import *\n",
    "from functions.utils import clean_text\n",
    "from functions.utils import load_embedding, create_embedding_weights, max_seq_len\n",
    "from keras.utils import to_categorical, plot_model\n",
    "import keras.backend as K\n",
    "import matplotlib as plt\n",
    "import models.models as models\n",
    "\n",
    "import functions.model_evaluation\n",
    "from functions.model_evaluation import *\n",
    "\n",
    "import functions.data_manipulation\n",
    "from functions.data_manipulation import *\n",
    "from keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading datasets\n",
    "train = pd.read_csv('data/train.txt', delimiter = '\\t')\n",
    "test = pd.read_csv('data/test.txt', delimiter = '\\t')\n",
    "test_ann = pd.read_csv('data/test_anno.txt', delimiter = '\\t')\n",
    "trial = pd.read_csv('data/trial.txt', delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data\n",
    "train['sentence_A'] = train['sentence_A'].apply(clean_text)\n",
    "train['sentence_B'] = train['sentence_B'].apply(clean_text)\n",
    "test['sentence_A'] = test['sentence_A'].apply(clean_text)\n",
    "test['sentence_B'] = test['sentence_B'].apply(clean_text)\n",
    "trial['sentence_A'] = trial['sentence_A'].apply(clean_text)\n",
    "trial['sentence_B'] = trial['sentence_B'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding the target feature\n",
    "lbl_enc = LabelEncoder()\n",
    "train['entailment_encoded'] = lbl_enc.fit_transform(train['entailment_judgment'])\n",
    "trial['entailment_encoded'] = lbl_enc.fit_transform(trial['entailment_judgment'])\n",
    "test_ann['entailment_encoded'] = lbl_enc.fit_transform(test['entailment_judgment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'word_embeddings/glove.6B.300d.txt'\n",
    "embeddings = load_embedding(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = len(embeddings) #200000\n",
    "sentences = (list(train['sentence_A']) + list(train['sentence_B']) + \n",
    "                       list(test['sentence_A']) + list(test['sentence_B'])+ \n",
    "                           list(trial['sentence_A']) + list(trial['sentence_B']))\n",
    "tokenize = Tokenizer(num_words = NUM_WORDS)\n",
    "tokenize.fit_on_texts(sentences)\n",
    "sent1_word_seq = tokenize.texts_to_sequences(train['sentence_A'])\n",
    "sent2_word_seq = tokenize.texts_to_sequences(train['sentence_B'])\n",
    "sent1_word_seq_test = tokenize.texts_to_sequences(test['sentence_A'])\n",
    "sent2_word_seq_test = tokenize.texts_to_sequences(test['sentence_B'])\n",
    "sent1_word_seq_trial = tokenize.texts_to_sequences(trial['sentence_A'])\n",
    "sent2_word_seq_trial = tokenize.texts_to_sequences(trial['sentence_B'])\n",
    "word_index = tokenize.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrix with the embedding weights\n",
    "embedding_dim = 300\n",
    "embedding_weights = create_embedding_weights(embeddings, embedding_dim, word_index, NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the maximum sequence length\n",
    "max_seq_length = max_seq_len(sent1_word_seq)\n",
    "max_seq_length = max_seq_len(sent2_word_seq, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent1_word_seq_test, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent2_word_seq_test, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent1_word_seq_trial, max_seq_length)\n",
    "max_seq_length = max_seq_len(sent2_word_seq_trial, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the sequences\n",
    "sent1_data = pad_sequences(sent1_word_seq, maxlen = max_seq_length)\n",
    "sent2_data = pad_sequences(sent2_word_seq, maxlen = max_seq_length)\n",
    "\n",
    "sent1_data_trial = pad_sequences(sent1_word_seq_trial, maxlen = max_seq_length)\n",
    "sent2_data_trial = pad_sequences(sent2_word_seq_trial, maxlen = max_seq_length)\n",
    "\n",
    "sent1_data_test = pad_sequences(sent1_word_seq_test, maxlen = max_seq_length)\n",
    "sent2_data_test = pad_sequences(sent2_word_seq_test, maxlen = max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2307"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_WORDS = len(embedding_weights)\n",
    "NUM_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = models(embedding_dim = embedding_dim,\n",
    "                   NUM_WORDS = NUM_WORDS,\n",
    "                   embedding_weights = embedding_weights,\n",
    "                   max_seq_length = max_seq_length,\n",
    "                   task = 'relatedness',\n",
    "                   dropout = .1,\n",
    "                   l2_reg=.0001\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 32, 300)      692100      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300)          721200      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "L1_distance (Lambda)            (None, 300)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 300)          0           L1_distance[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "similarity_task (Dense)         (None, 1)            301         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,413,601\n",
      "Trainable params: 721,501\n",
      "Non-trainable params: 692,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = m1.siames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/15\n",
      "4500/4500 [==============================] - 65s 14ms/step - loss: 0.2124 - pearson_correlation: 0.8899 - val_loss: 0.3572 - val_pearson_correlation: 0.7479\n",
      "Epoch 2/15\n",
      "4500/4500 [==============================] - 68s 15ms/step - loss: 0.1924 - pearson_correlation: 0.9046 - val_loss: 0.3781 - val_pearson_correlation: 0.7599\n",
      "Epoch 3/15\n",
      "4500/4500 [==============================] - 69s 15ms/step - loss: 0.1739 - pearson_correlation: 0.9180 - val_loss: 0.3137 - val_pearson_correlation: 0.7857\n",
      "Epoch 4/15\n",
      "4500/4500 [==============================] - 68s 15ms/step - loss: 0.1438 - pearson_correlation: 0.9261 - val_loss: 0.3121 - val_pearson_correlation: 0.7740\n",
      "Epoch 5/15\n",
      "4500/4500 [==============================] - 67s 15ms/step - loss: 0.1429 - pearson_correlation: 0.9293 - val_loss: 0.3247 - val_pearson_correlation: 0.7812\n",
      "Epoch 6/15\n",
      "4500/4500 [==============================] - 67s 15ms/step - loss: 0.1369 - pearson_correlation: 0.9318 - val_loss: 0.3192 - val_pearson_correlation: 0.7719\n",
      "Epoch 7/15\n",
      "4500/4500 [==============================] - 67s 15ms/step - loss: 0.1216 - pearson_correlation: 0.9431 - val_loss: 0.3852 - val_pearson_correlation: 0.7837\n",
      "Epoch 8/15\n",
      "4500/4500 [==============================] - 71s 16ms/step - loss: 0.1128 - pearson_correlation: 0.9470 - val_loss: 0.3223 - val_pearson_correlation: 0.7759\n",
      "Epoch 9/15\n",
      "4500/4500 [==============================] - 70s 16ms/step - loss: 0.1115 - pearson_correlation: 0.9465 - val_loss: 0.3125 - val_pearson_correlation: 0.7772\n",
      "Epoch 10/15\n",
      "4500/4500 [==============================] - 70s 15ms/step - loss: 0.1045 - pearson_correlation: 0.9533 - val_loss: 0.2996 - val_pearson_correlation: 0.7754\n",
      "Epoch 11/15\n",
      "4500/4500 [==============================] - 70s 16ms/step - loss: 0.0950 - pearson_correlation: 0.9586 - val_loss: 0.3275 - val_pearson_correlation: 0.7784\n",
      "Epoch 12/15\n",
      "4500/4500 [==============================] - 75s 17ms/step - loss: 0.0960 - pearson_correlation: 0.9575 - val_loss: 0.3194 - val_pearson_correlation: 0.7714\n",
      "Epoch 13/15\n",
      "4500/4500 [==============================] - 71s 16ms/step - loss: 0.0909 - pearson_correlation: 0.9581 - val_loss: 0.3557 - val_pearson_correlation: 0.7680\n",
      "Epoch 14/15\n",
      "4500/4500 [==============================] - 69s 15ms/step - loss: 0.0926 - pearson_correlation: 0.9589 - val_loss: 0.3234 - val_pearson_correlation: 0.7849\n",
      "Epoch 15/15\n",
      "4500/4500 [==============================] - 71s 16ms/step - loss: 0.0789 - pearson_correlation: 0.9619 - val_loss: 0.3580 - val_pearson_correlation: 0.7729\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([sent1_data, sent2_data], train['relatedness_score'], batch_size = 10, \n",
    "                 epochs = 15,\n",
    "                 validation_data = ([sent1_data_trial, sent2_data_trial], trial['relatedness_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = model.predict([sent1_data_test, sent2_data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [i[0] for i in k]\n",
    "y_true = test_ann['relatedness_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson, spearman, mean_abs_deviation = evaluate_relatedness(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson: 0.8321203296074714\n",
      "spearman: 0.7837459793551984\n",
      "mean_abs_deviation: 15.854661095719488% \n"
     ]
    }
   ],
   "source": [
    "print(\"pearson: \"+str(pearson[0]))\n",
    "print(\"spearman: \"+str(spearman[0]))\n",
    "print(\"mean_abs_deviation: \"+str(mean_abs_deviation)+ \"% \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hkpat\\Masters\\NLP\\Assignment2_final\\submission\\functions\\data_manipulation.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['relatedness_score']= relatedness_result\n"
     ]
    }
   ],
   "source": [
    "siamese_df = map_relatedness(test,y_pred,cols = ['pair_ID']  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv(siamese_df, file_name ='entailment_relatedness/relatedness/siamese_lstm_relatedness.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### siamese CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = models.models(embedding_dim = embedding_dim,\n",
    "                  NUM_WORDS = NUM_WORDS,\n",
    "                   embedding_weights = embedding_weights,\n",
    "                   max_seq_length = max_seq_length,\n",
    "                   task = 'relatedness',\n",
    "                   dropout = .1,\n",
    "                   l2_reg=.0001\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 250)          731550      input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 250)          0           sequential_7[1][0]               \n",
      "                                                                 sequential_7[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relatedness_task (Dense)        (None, 1)            251         lambda_10[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 731,801\n",
      "Trainable params: 39,701\n",
      "Non-trainable params: 692,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "siames_cnn = m1.cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3258 - val_loss: 1.0038\n",
      "Epoch 2/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3297 - val_loss: 1.0370\n",
      "Epoch 3/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3281 - val_loss: 0.9650\n",
      "Epoch 4/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3171 - val_loss: 1.0542\n",
      "Epoch 5/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3210 - val_loss: 0.9425\n",
      "Epoch 6/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3146 - val_loss: 0.9547\n",
      "Epoch 7/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3063 - val_loss: 0.9182\n",
      "Epoch 8/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3138 - val_loss: 1.0033\n",
      "Epoch 9/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3142 - val_loss: 0.9701\n",
      "Epoch 10/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3097 - val_loss: 0.9932\n",
      "Epoch 11/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3109 - val_loss: 0.9609\n",
      "Epoch 12/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3065 - val_loss: 0.9634\n",
      "Epoch 13/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3098 - val_loss: 0.9319\n",
      "Epoch 14/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3031 - val_loss: 0.8997\n",
      "Epoch 15/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3054 - val_loss: 0.9868\n",
      "Epoch 16/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3085 - val_loss: 0.9321\n",
      "Epoch 17/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3036 - val_loss: 0.9742\n",
      "Epoch 18/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3002 - val_loss: 0.9677\n",
      "Epoch 19/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3054 - val_loss: 0.9818\n",
      "Epoch 20/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.2980 - val_loss: 0.9798\n",
      "Epoch 21/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3013 - val_loss: 0.9719\n",
      "Epoch 22/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3080 - val_loss: 0.9602: 0.306\n",
      "Epoch 23/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.3057 - val_loss: 1.0206\n",
      "Epoch 24/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.2988 - val_loss: 0.9583\n",
      "Epoch 25/25\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.2963 - val_loss: 0.9755\n"
     ]
    }
   ],
   "source": [
    "hist = siamese_cnn.fit([sent1_data, sent2_data], train['relatedness_score'], batch_size = 10, \n",
    "                 epochs = 25,\n",
    "                 validation_data = ([sent1_data_trial, sent2_data_trial], trial['relatedness_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = siamese_cnn.predict([sent1_data_test, sent2_data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [i[0] for i in k]\n",
    "y_true = test_ann['relatedness_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson, spearman, mean_abs_deviation = evaluate_relatedness(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson: 0.42540778986934513\n",
      "spearman: 0.356766587848538\n",
      "mean_abs_deviation: 29.027313147387307% \n"
     ]
    }
   ],
   "source": [
    "print(\"pearson: \"+str(pearson[0]))\n",
    "print(\"spearman: \"+str(spearman[0]))\n",
    "print(\"mean_abs_deviation: \"+str(mean_abs_deviation)+ \"% \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_df = map_relatedness(test,y_pred,cols = ['pair_ID']  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv(cnn_df, file_name ='entailment_relatedness/relatedness/cnn_relatedness.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(max_seq_length,))\n",
    "input_2 = Input(shape=(max_seq_length,))\n",
    "\n",
    "left_input = Embedding(input_dim=NUM_WORDS,\n",
    "               output_dim=embedding_dim,\n",
    "               weights=[embedding_weights],\n",
    "               input_length=max_seq_length,\n",
    "               trainable=False)(input_1)\n",
    "left_input = TimeDistributed(Dense(300, activation='relu'))(left_input)\n",
    "left_input = Lambda(lambda x: K.max(x, axis=1), output_shape=(300,))(left_input)\n",
    "\n",
    "right_input = Embedding(input_dim=NUM_WORDS,\n",
    "               output_dim=300,\n",
    "               weights=[embedding_weights],\n",
    "               input_length=max_seq_length,\n",
    "               trainable=False)(input_2)\n",
    "right_input = TimeDistributed(Dense(300, activation='relu'))(right_input)\n",
    "right_input = Lambda(lambda x: K.max(x, axis=1), output_shape=(300,))(right_input)\n",
    "\n",
    "x = concatenate([left_input, right_input])\n",
    "x = Dense(200, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "out = Dense(1, activation='selu')(x)\n",
    "\n",
    "model = Model(inputs=[input_1, input_2], outputs=out)\n",
    "model.compile(loss='mse', optimizer=Adam(0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "4500/4500 [==============================] - 9s 2ms/step - loss: 12.2873 - val_loss: 7.7472\n",
      "Epoch 2/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 7.4577 - val_loss: 5.7382\n",
      "Epoch 3/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 3.9398 - val_loss: 2.3559\n",
      "Epoch 4/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 2.5353 - val_loss: 1.5563\n",
      "Epoch 5/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 2.1816 - val_loss: 1.2199\n",
      "Epoch 6/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 2.0159 - val_loss: 1.2513\n",
      "Epoch 7/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.9616 - val_loss: 1.4257\n",
      "Epoch 8/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.8563 - val_loss: 1.0676\n",
      "Epoch 9/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.7881 - val_loss: 1.1581\n",
      "Epoch 10/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.8121 - val_loss: 1.1020\n",
      "Epoch 11/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.7390 - val_loss: 1.1124\n",
      "Epoch 12/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.6864 - val_loss: 1.1342\n",
      "Epoch 13/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.6307 - val_loss: 1.0687\n",
      "Epoch 14/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.5679 - val_loss: 1.2067\n",
      "Epoch 15/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.5343 - val_loss: 1.0852\n",
      "Epoch 16/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.5168 - val_loss: 1.0398\n",
      "Epoch 17/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.4154 - val_loss: 1.0466\n",
      "Epoch 18/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.4088 - val_loss: 1.0572\n",
      "Epoch 19/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.3825 - val_loss: 1.0355\n",
      "Epoch 20/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.3867 - val_loss: 1.0727\n",
      "Epoch 21/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.3183 - val_loss: 1.1570\n",
      "Epoch 22/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.3154 - val_loss: 1.0502\n",
      "Epoch 23/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.3159 - val_loss: 1.1274\n",
      "Epoch 24/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.2761 - val_loss: 1.0425\n",
      "Epoch 25/25\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 1.2647 - val_loss: 0.9964\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([sent1_data, sent2_data], train['relatedness_score'], batch_size = 10, \n",
    "                 epochs = 25,\n",
    "                 validation_data = ([sent1_data_trial, sent2_data_trial], trial['relatedness_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = model.predict([sent1_data_test, sent2_data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [i[0] for i in k]\n",
    "y_true = test_ann['relatedness_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson, spearman, mean_abs_deviation = evaluate_relatedness(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson: 0.22068082651994758\n",
      "spearman: 0.18324760312426733\n",
      "mean_abs_deviation: 32.146169094015534% \n"
     ]
    }
   ],
   "source": [
    "print(\"pearson: \"+str(pearson[0]))\n",
    "print(\"spearman: \"+str(spearman[0]))\n",
    "print(\"mean_abs_deviation: \"+str(mean_abs_deviation)+ \"% \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_rnn = map_relatedness(test,y_pred,cols = ['pair_ID']  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv(deep_rnn, file_name ='entailment_relatedness/relatedness/deeprnn_relatedness.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 64, 300)      692100      main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64, 300)      0           embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_fwd (LSTM)                 (None, 64, 125)      213000      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_bwd (LSTM)                 (None, 64, 125)      213000      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64, 250)      0           lstm_fwd[0][0]                   \n",
      "                                                                 lstm_bwd[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "d_bilstm (Dropout)              (None, 64, 250)      0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flat_h_star (Flatten)           (None, 16000)        0           d_bilstm[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 1)            16001       flat_h_star[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,134,101\n",
      "Trainable params: 442,001\n",
      "Non-trainable params: 692,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hkpat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model([<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "main_input = Input(shape=(max_seq,), dtype='int32', name='main_input') #(N,70)\n",
    "#x = Embedding(output_dim=opts['emb'], input_dim=len(VOCABULARY.keys())+1, input_length=N, name='x')(main_input)\n",
    "\n",
    "x = Embedding(\n",
    "            NUM_WORDS,\n",
    "            embedding_dim,\n",
    "            weights = [embedding_weights], \n",
    "            input_length = max_seq,\n",
    "            trainable = False)(main_input)\n",
    "\n",
    "drop_out = Dropout(0.3, name='dropout')(x) # 70,50\n",
    "lstm_fwd = LSTM(125, return_sequences=True, name='lstm_fwd')(drop_out)\n",
    "lstm_bwd = LSTM(125, return_sequences=True, go_backwards=True, name='lstm_bwd')(drop_out)\n",
    "#70,100\n",
    "bilstm = Concatenate()([lstm_fwd,lstm_bwd])\n",
    "#70,200\n",
    "drop_out = Dropout(0.1, name=\"d_bilstm\")(bilstm)\n",
    "flat_h_star = Flatten(name=\"flat_h_star\")(drop_out)\n",
    "out = Dense(1, activation='selu')(flat_h_star)\n",
    "\n",
    "model = Model([main_input], output=out)\n",
    "model.summary()\n",
    "model.compile(loss='mse',optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = np.concatenate((sent1_data, sent2_data),axis= 1)\n",
    "sent_trial = np.concatenate((sent1_data_trial, sent2_data_trial), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/15\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 1.3040 - val_loss: 1.3265\n",
      "Epoch 2/15\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 1.0947 - val_loss: 1.0727\n",
      "Epoch 3/15\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 1.0016 - val_loss: 1.0165\n",
      "Epoch 4/15\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.9938 - val_loss: 1.0393\n",
      "Epoch 5/15\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.9282 - val_loss: 1.0341\n",
      "Epoch 6/15\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.9078 - val_loss: 1.0706\n",
      "Epoch 7/15\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.8813 - val_loss: 1.0164\n",
      "Epoch 8/15\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.8289 - val_loss: 1.0491\n",
      "Epoch 9/15\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.7949 - val_loss: 1.0399\n",
      "Epoch 10/15\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.7642 - val_loss: 1.1357\n",
      "Epoch 11/15\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.7300 - val_loss: 1.0826\n",
      "Epoch 12/15\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.6817 - val_loss: 1.1327\n",
      "Epoch 13/15\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.6482 - val_loss: 1.1415\n",
      "Epoch 14/15\n",
      "4500/4500 [==============================] - 15s 3ms/step - loss: 0.6323 - val_loss: 1.2262\n",
      "Epoch 15/15\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.5929 - val_loss: 1.1684\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(sent, train['relatedness_score'], batch_size = 32, \n",
    "                 epochs = 15,\n",
    "                 validation_data = (sent_trial, trial['relatedness_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_test = np.concatenate((sent1_data_test, sent2_data_test), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = model.predict(sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [i[0] for i in k]\n",
    "y_true = test_ann['relatedness_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson: 0.27552558822749795\n",
      "spearman: 0.20059533062058138\n",
      "mean_abs_deviation: 31.209168436051787% \n"
     ]
    }
   ],
   "source": [
    "print(\"pearson: \"+str(pearson[0]))\n",
    "print(\"spearman: \"+str(spearman[0]))\n",
    "print(\"mean_abs_deviation: \"+str(mean_abs_deviation)+ \"% \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hkpat\\Masters\\NLP\\Assignment2_final\\submission\\functions\\data_manipulation.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['relatedness_score']= relatedness_result\n"
     ]
    }
   ],
   "source": [
    "bidir_df = map_relatedness(test,y_pred,cols = ['pair_ID']  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv(bidir_df, file_name ='entailment_relatedness/relatedness/bidirectional_lstm_relatedness.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
